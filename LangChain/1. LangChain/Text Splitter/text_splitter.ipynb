{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab80cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='Memory Hierarchy: Caches and Virtual Memory\\n David Castro-Perez\\nNovember 10, 2025\\nContents\\n1 The Memory Hierarchy in Context 3\\n1.1 Definition and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 The Principle of Locality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.3 Structure of the Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.4 Quantitative View: Average Memory Access Time (AMAT) . . . . . . . . . . . . . . 6\\n1.5 The Impact on CPU Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.6 Trade-offs in the Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2 Caches: Organization, Operation, and Performance 8\\n2.1 From Concept to Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Cache Structure and Address Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3 Mapping Strategies: Where Does Each Block Go? . . . . . . . . . . . . . . . . . . . 10\\n2.4 Replacement Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5 Write Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.6 Types of Misses (3C Model) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.7 Quantitative Analysis: Average Memory Access Time. . . . . . . . . . . . . . . . . . 14\\n2.8 Pipeline Interaction: Instruction vs. Data Cache . . . . . . . . . . . . . . . . . . . . 15\\n2.9 Summary of Section 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3 Exercises: Caches 18\\n3.1 Purpose and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.2 Worked Example 3.1 — Direct-Mapped Cache Indexing and Miss Pattern . . . . . . 18\\n3.3 Worked Example 3.2 — Conflict Misses in a Direct-Mapped Cache . . . . . . . . . . 19\\n3.4 Worked Example 3.3 — AMAT Under Different Parameters . . . . . . . . . . . . . . 20\\n3.5 Worked Example 3.4 — Address Fields in a Direct-Mapped Cache . . . . . . . . . . 20\\n3.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4 Virtual Memory and Address Translation 22\\n4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4.2 Pages and Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4.3 The Page Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4.4 The Translation Lookaside Buffer (TLB) . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n4.5 Page Faults and the Role of the Operating System . . . . . . . . . . . . . . . . . . . 27\\n4.6 Interaction with the Cache Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.7 Summary of Section 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n1'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 1, 'page_label': '2'}, page_content='5 Assembly Programming and the Integrated Memory View 30\\n5.1 Why Addressing Modes Matter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n5.2 MIPS Addressing Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n5.3 x86_64 Addressing Modes (for comparison) . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.4 Efficiency and the Memory Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n6 Exercises: Virtual Memory 32\\n6.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n7 Integration and Summary 34\\n7.1 The Complete Memory Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.2 Address Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.3 Design Trade-offs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n7.4 Summary of Section 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\nA Reference Tables 35\\nA.1 MIPS vs x86_64 Addressing Summary . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\nA.2 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The basic structure of a memory hierarchy. By implementing the memory system \\nas a hierarchy, the user has the illusion of a memory that is as large as the largest level of the \\nhierarchy, but can be accessed as if it were all built from the fastest memory. (Fig 5.1 in [Patterson \\n& Hennessy])\\nBased on (must-read): Patterson & Hennessy, Computer Organization and Design, MIPS \\nEdition, Chapter 5: “Large and Fast: Exploiting Memory Hierarchy”.\\n1 The Memory Hierarchy in Context\\n1.1 Definition and Motivation\\nA memory hierarchy is a structure that uses multiple levels of memory. As the distance from the \\nprocessor increases, the size of each level increases, the access time also increases, and the cost per \\nbit decreases. This arrangement allows the computer to appear as though it has both large capacity \\nand high speed.\\nThe performance gap. Since the 1980s, processor clock rates have improved by several orders \\nof magnitude, while DRAM latency has improved only modestly. A modern CPU core can exe\\xad\\ncute billions of operations per second, but each main-memory access may take hundreds of cycles. \\nWithout some intermediate storage, the processor would spend most of its time waiting for data.\\nAn intuitive analogy. Imagine a chef (the CPU) preparing meals in a kitchen. If every ingredient \\nwere stored in a warehouse miles away (the main memory), the chef would spend most of the day \\nwalking back and forth rather than cooking. To stay productive, the chef keeps the most commonly \\nused ingredients within arm’s reach (registers and caches), less frequently used items in a nearby \\npantry (main memory), and rare ingredients in the distant warehouse (disk or SSD). The closer the \\nstorage, the faster it is-but the less it can hold.\\nWhy a hierarchy works. Even though only a small fraction of the total data fits in the top \\nlevels, programs often reuse the same data and instructions repeatedly. If those active portions can \\n3'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 3, 'page_label': '4'}, page_content='Figure 2: Every pair of levels in the memory hierarchy can be thought of as having an \\nupper and lower level. Within each level, the unit of information that is present or not is called \\na block or a line. Usually we transfer an entire block when we copy something between levels. (Fig \\n5.2 in [Patterson & Hennessy])\\nstay near the processor, the system will behave as if all memory were fast.\\n1.2 The Principle of Locality\\nThe idea that makes the hierarchy work is the principle of locality. Empirically, programs do not \\naccess memory randomly; instead, they tend to use the same data and nearby data for extended \\nperiods.\\n4'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 4, 'page_label': '5'}, page_content='Figure 3: This diagram shows the structure of a memory hierarchy: as the distance from the \\nprocessor increases, so does the size.This structure, with the appropriate operating mechanisms, \\nallows the processor to have an access time that is determined primarily by level 1 of the hierarchy \\nand yet have a memory as large as level n. Maintaining this illusion is the subject of Week 6 lectures \\n(Fig 5.3 in [Patterson & Hennessy])\\nTemporal locality If a location is referenced at one time, it is likely to be referenced again soon. \\nTypical examples include variables reused within a loop, recently called procedures, or ele\\xad\\nments of a stack frame that are touched repeatedly.\\nSpatial locality If a location is referenced, nearby locations are likely to be referenced soon. For \\ninstance, instructions are fetched sequentially, and arrays are traversed element by element.\\nBoth forms of locality imply that recently and nearby used information is far more valuable than \\ndistant information. This is the key reason that small, fast memories can dramatically accelerate \\ncomputation.\\n1.3 Structure of the Hierarchy\\nEach level of the hierarchy acts as a cache for the level below it. The processor always interacts \\nwith the highest level (registers and caches), and only if data is missing does it access the next level.\\nLevels.\\n• Registers: a few dozen to a few hundred bytes, accessed every clock cycle.\\n• Caches: small SRAM memories (tens of kilobytes to a few megabytes) that automatically \\nkeep frequently used data close to the CPU.\\n5'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6'}, page_content='• Main memory (DRAM): gigabytes of capacity, slower but inexpensive per bit.\\n• Secondary storage (SSD, HDD): terabytes of capacity, but millions of times slower than \\nregisters.\\nEach level exploits locality by storing blocks of data that are likely to be reused soon.\\nBlocks (or cache lines). Information moves between adjacent levels in fixed-size chunks called \\nblocks (or cache lines). A block is typically 16 – 128 bytes. When the CPU requests one byte or \\nword, the entire block containing it is fetched into the higher level. If the program later accesses \\nanother word from that same block, the request will be satisfied quickly—this is spatial locality in \\naction.\\nHits and misses. If the requested data is already present in the current level, we have a hit; \\notherwise, we have a miss. The hit rate is the fraction of accesses served by that level, usually 90 \\n– 99 %. The miss penalty is the time required to fetch the data from the next level.\\nControl of data movement. Movement between levels is automatic and invisible to the pro\\xad\\ngrammer. Hardware caches manage blocks using address tags; operating systems manage pages \\nusing virtual-memory tables. In both cases, recently used information is retained while less useful \\ndata is displaced.\\n1.4 Quantitative View: Average Memory Access Time (AMAT)\\nTo evaluate how effective a hierarchy is, architects use the Average Memory Access Time \\n(AMAT):\\nAMAT = Hit time + Miss rate × Miss penalty.\\n• Hit time — the time to access data on a hit (includes tag check).\\n• Miss rate — the fraction of accesses that miss in this level.\\n• Miss penalty — the extra time to retrieve the block from the next level.\\nFor multi-level hierarchies, AMAT is computed recursively: the average access time of one level \\nbecomes the miss penalty of the level above.\\nWorked Example 1.1 — Quantifying the Effect of a Cache\\nSuppose main memory access takes 100 ns. We add an L1 cache that can serve a hit in 2 ns and \\nhas a 95 % hit rate. Then:\\nAMAT = 2 + 0.05 × 100 = 7 ns.\\nThe cache reduces average access time by a factor of about 14.\\nTwo-level AMAT derivation. Let L1 have hit time t1 and miss rate m1. Let L2 have hit time \\nt2 and miss rate m2 (fraction of L1 misses), and main memory time tM.\\nAMAT = t1 + m1\\n(︂\\nt2 + m2 · tM\\n)︂\\n.\\nExample: t1 = 1, m1 = 0.05, t2 = 10, m2 = 0.10, tM = 100 ⇒ AMAT = 2.0 cycles.\\n6'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7'}, page_content=\"Worked Example 1.2 — Adding a Second Level\\nNow add an L2 cache that handles 90 % of the L1 misses in 10 ns. Only 10 % of L1 misses go to \\nmain memory (100 ns):\\nAMAT = 2 + 0.05(10 + 0.10 × 100) = 2 + 0.05 × 20 = 3 ns.\\nAdding L2 more than halves the average access time again.\\n1.5 The Impact on CPU Performance\\nEvery cache miss introduces extra cycles in the processor pipeline. If each instruction on average \\nperforms one memory access, we can express the total cycles per instruction (CPI) as:\\nCPItotal = CPIbase + (misses/instruction) × miss penalty.\\nExample. Base CPI = 1, L1 miss rate = 2 %, miss penalty = 50 cycles:\\nCPItotal = 1 + 0.02 × 50 = 2.\\nEven a tiny miss rate doubles total execution time. This is why architectural effort (multi-level \\ncaches, prefetching) and software effort (data locality) are both critical.\\n1.6 Trade-offs in the Hierarchy\\nEach level balances five competing factors:\\n• Latency: how fast each access is.\\n• Bandwidth: how much data per second can be transferred.\\n• Capacity: total storage size.\\n• Cost per bit: economic efficiency of the technology.\\n• Complexity: hardware or software management overhead.\\nFast memories (SRAM) are small and expensive; slow memories (DRAM, SSD) are large and \\ncheap. By combining them, the system behaves like a single large, fast, affordable memory —\\nprovided locality holds.\\nAdditional Contents (non-examinable): Why Programmers Should Care\\nThis short section is not examinable. It shows the practical relevance of locality for anyone writing \\nprograms.\\nEven high-level languages are affected by the memory hierarchy. Consider the following Python \\nloop:\\nimport time\\nfrom array import array\\nN = 10000\\nstride = 10000\\ndata = array('d', (float(i) for i in range(stride * N)))\\ns = 0.0\\nt0 = time.perf_counter()\\n7\"), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 7, 'page_label': '8'}, page_content='for i in range(N):                  # visit consecutive elements\\n    s += data[i]\\nt1 = time.perf_counter()\\nprint(\"Sequential time (good spatial locality):\", t1 - t0, \"seconds\")\\nt0 = time.perf_counter()\\nfor i in range(0, stride * N, stride):   # jump by \\'stride\\' each time\\n    s += data[i]\\nt1 = time.perf_counter()\\nprint(\"Strided time (bad spatial locality):\", t1 - t0, \"seconds\")\\nWhen stride = 1, elements are accessed consecutively; each cache line fetched from memory \\ncontains many useful values. When stride = 64, each access touches a different cache line, forcing \\nmany misses. The program may slow down by an order of magnitude even though the algorithm is \\nidentical. This is a simple, visible effect of spatial locality on performance.\\n1.7 Summary\\n• The memory hierarchy combines small, fast, costly memories near the processor with large, \\nslow, cheap memories farther away.\\n• The hierarchy’s effectiveness depends on the principle of locality.\\n• Information moves between levels in fixed-size blocks (cache lines).\\n• Performance is measured by the Average Memory Access Time (AMAT).\\n• Even small miss rates can significantly slow execution.\\n• (Non-examinable) Awareness of locality helps programmers write faster, more cache-friendly \\ncode.\\n2 Caches: Organization, Operation, and Performance\\n2.1 From Concept to Hardware\\nA cache is a small, fast memory built from static RAM (SRAM) that stores copies of recently used \\nportions of main memory (DRAM). It sits between the CPU and main memory and acts as a buffer \\nto exploit the principle of locality:\\nAMAT = Hit time + Miss rate × Miss penalty.\\nWhen the processor issues a load or store:\\n1. The cache checks if the requested address is already present (a hit).\\n2. If so, data are returned immediately.\\n3. If not (a miss), the entire block of memory containing that address is fetched from the next \\nlower level and stored in the cache.\\n8'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9'}, page_content='Figure 4: Cache contents immediately before (left) and after (right) an access to a word xn in \\nmemory that is not initially in the cache. The reference causes a miss that forces the cache to fetch \\nxn from memory and insert it into the cache (Fig 5.7 [Patterson & Hennesy]).\\nExtra (non-examinable) SRAM stands for Static RAM, and these are have faster access times \\n(usually 1 cycle). DRAM stands for Dynamic RAM, and these memories have slower access times. \\nSRAMs use 8 transistors per bit of information, whereas DRAMs use capacitors to store a cell \\nvalues, and one transistor per bit to read or write the value stored by a capacitor. Because \\nDRAMs only use one transistor per bit, they are denser and cheaper than SRAMs. However, \\nsince DRAMs contents store data in capacitors, and these eventually lose their charge, they need \\nto be refreshed periodically (i.e. read and written back).\\n2.2 Cache Structure and Address Fields\\nA cache holds data in fixed-size blocks (or lines), each containing B bytes. Each block corresponds \\nto a contiguous region of main memory.\\nLet\\nC = cache capacity (bytes), B = block size (bytes), A = associativity (ways).\\nThen the number of sets is\\nS = C\\nB × A.\\nEach cache entry stores:\\n• a data block (B bytes),\\n9'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 9, 'page_label': '10'}, page_content='• a tag (the high-order address bits),\\n• a valid bit,\\n• optionally a dirty bit (for write-back caches).\\nAddress breakdown. Given an m-bit address, the fields are:\\nTag⏞⏟⏟⏞\\nidentifies memory block\\n| Index⏞ ⏟⏟ ⏞\\nselects set\\n| Block offset⏞ ⏟⏟ ⏞\\nlocates byte within block\\n.\\nOffset bits = log2(B), Index bits = log2(S), Tag bits = m − (offset + index).\\nWorked Example 2.1 — Address Breakdown\\nA 4 KB cache, 16 B blocks, direct-mapped, 32-bit addresses:\\nS = 4096\\n16 = 256 = 28 ⇒ offset = 4, index = 8, tag = 20.\\nAddress 0x12AB34CD → lowest 4 bits = offset, next 8 bits = index, top 20 bits = tag.\\nWe can expand the example to see precisely how the address fields are divided.\\nGiven a 32-bit address 0x12AB34CD:\\n0x12AB34CD = 0001 0010 1010 1011 0011 0100 1100 11012\\nWe divide the 32-bit address accordingly (from least to most significant bits):\\n Field  Bit positions  Binary  Hex  Decimal  Meaning\\n Offset  [3..0]  1101  D  13  Byte within 16 B block\\n Index  [11..4]  0100 1100  4C  76  Cache line number\\n Tag  [31..12]  0001 0010 1010 1011 0011  12AB3  —  Identifies memory block\\nNotes:\\n• The index crosses a byte boundary: bits [11..8] are the low nibble of 0x34 (= 0x4); bits [7..4] \\nare the high nibble of 0xCD (= 0xC). Together they form 0x4C.\\n• Block base address = 0x12AB34CD& 0xFFFFFFF0 = 0x12AB34C0.\\n• Cache line index = 0x4C = 76.\\n• Tag stored in that line = 0x12AB3.\\nTag = 0x12AB3, Index = 0x4C, Offset = 0xD.\\nThis explicit division will be used repeatedly when analyzing cache hits and misses in Section 3.\\n2.3 Mapping Strategies: Where Does Each Block Go?\\nThe mapping policy determines which cache location(s) can store a given memory block.\\n10'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11'}, page_content='Figure 5: A direct-mapped cache with eight entries showing the addresses of memory words between \\n0 and 31 that map to the same cache locations. Because there are eight words in the cache, an \\naddress X maps to the direct-mapped cache word X%8. That is, the low-order log2(8) = 3 bits \\nare used as the cache index. Thus, addresses 00001two, 0b01001, 0b10001, and 0b11001 all map to \\nentry 0b001 of the cache, while addresses 0b00101, 0b01101, 0b10101, and 0b11101 all map to entry \\n0b101 of the cache. (Fig 5.8 of [Patterson & Hennessy]).\\n1. Direct-Mapped. Each memory block maps to exactly one cache line:\\nline index = (block address) mod N,\\nwhere N = number of lines.\\nHardware:\\n1. Extract index from address → select line.\\n2. Compare stored tag with address tag.\\n3. If equal and valid → hit; else → miss → fetch block → overwrite line.\\nThis is fast (one comparator) but vulnerable to conflict misses.\\nAs an example of how a direct-mapped cache works, consider a cache with 8 lines (indexes 0b000 \\nto 0b111) and a block size of one word. Each memory reference is given as a word address, so the \\ncache index is simply (address mod 8). The table below traces a short sequence of addresses and \\nshows for each one:\\n1. the decimal and binary forms of the reference,\\n2. whether the reference is a hit or miss,\\n11'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 11, 'page_label': '12'}, page_content='Figure 6: Example of cache contents after each reference request. (Fig 5.9 in Patterson & Hennesy).\\n3. and the cache line (block) that it maps to.\\nEach line of the table corresponds to one snapshot in the illustration (Fig. 6a – f). Notice that \\naddresses separated by multiples of 8 map to the same cache block, causing conflict misses in a \\ndirect-mapped cache. This simple example captures the essence of cache replacement and locality \\nin direct mapping.\\nDecimal ad\\xad\\ndress of ref\\xad\\nerence\\nBinary address \\nof reference\\nHit or miss \\nin cache\\nAssigned cache block \\n(where found or placed)\\n22 0b10110 miss (Fig. 6b) (0b10110 mod 8) = 0b110\\n26 0b11010 miss (Fig. 6c) (0b11010 mod 8) = 0b010\\n22 0b10110 hit (0b10110 mod 8) = 0b110\\n26 0b11010 hit (0b11010 mod 8) = 0b010\\n16 0b10000 miss (Fig. 6d) (0b10000 mod 8) = 0b000\\n3 0b00011 miss (Fig. 6e) (0b00011 mod 8) = 0b011\\n16 0b10000 hit (0b10000 mod 8) = 0b000\\n18 0b10010 miss (Fig. 6f) (0b10010 mod 8) = 0b010\\n16 0b10000 hit (0b10000 mod 8) = 0b000\\n12'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13'}, page_content='2. Set-Associative. Each set contains multiple ways. A memory block maps to exactly one set \\nbut may occupy any of its ways.\\nIf A=number of ways, then\\nS = C\\nB × A.\\nThe index chooses one of the S sets; within that set, A entries exist.\\nInternal structure (the hardware “table” ). Each set is a small table of A entries. Each \\nentry stores:\\n• its tag (high-order bits of the address),\\n• a valid and dirty bit,\\n• the data block.\\nOn every access:\\n1. The index bits select one set.\\n2. All A tags in that set are read simultaneously and compared (using A comparators).\\n3. If one matches and valid = 1 → hit.\\n4. Otherwise → miss → fetch block from lower level. If all ways are full, choose one line to evict.\\nHence, each set physically contains a tiny parallel table mapping tag → data. Associativity \\nmeans there are multiple such possible entries per index.\\nExample — 2-Way Cache Lookup. 4 KB cache, 16 B blocks, 2-way associative: S =\\n4096/(16×2) = 128 sets. Each set holds 2 entries:\\n[Tag0, Valid0, Data0], [Tag1, Valid1, Data1].\\nIf the index = 37, both tags of set 37 are compared with the requested tag. If neither matches, one \\nentry is evicted and replaced.\\nAssociativity reduces conflict misses because several competing blocks with the same index can \\ncoexist within a set.\\n3. Fully Associative.  This is a special case with S = 1. Every block can occupy any line; all \\ntags (hundreds) are compared in parallel. Hardware cost grows with number of comparators, so \\nthis is used only for small caches such as the TLB.\\n2.4 Replacement Policies\\nWhen a set is full and a miss occurs, one of its A entries must be evicted.\\nLeast Recently Used (LRU): replace the block not accessed for the longest time. Perfect for \\nsmall A (2 – 4). For 2-way caches, a single “recently used” bit per set suffices.\\nRandom: choose any way at random; simple and almost as effective for high A.\\nPseudo-LRU: approximate LRU using a binary tree of bits (used in 8-way L2 caches).\\nExample — 2-Way LRU. Each set has one “use” bit: when way 0 is used, mark 0 → recent; \\nwhen way 1 is used, mark 1 → recent. On a miss, evict the least-recently used way.\\n2.5 Write Policies\\nHow and when writes propagate to main memory:\\n13'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14'}, page_content='Figure 7: Direct mapped, set associative, vs fully associative. The location of a memory \\nblock whose address is 12 in a cache with eight blocks varies for direct-mapped, set-associative, \\nand fully associative placement.In direct-mapped placement, there is only one cache block where \\nmemory block 12 can be found, and that block is given by (12 modulo 8) = 4. In a two-way set-\\nassociative cache, there would be four sets, and memory block 12 must be in set (12 mod 4) = 0; the \\nmemory block could be in either element of the set. In a fully associative placement, the memory \\nblock for block address 12 can appear in any of the eight cache blocks. (Fig. 5.14 in [Patterson & \\nHennessy]).\\nWrite-through. Every write updates both cache and next level. +: simpler consistency. –: more \\nmemory traffic.\\nWrite-back. Write only modifies the cache copy; mark entry ’  s dirty bit. When evicted, the \\nwhole block is written to lower memory. +: fewer writes to memory. –: requires dirty-bit tracking \\nand write-back on replacement.\\nWrite-allocate vs. No-write-allocate. On a write miss:\\n• Write-allocate: fetch block into cache, then write → used with write-back.\\n• No-write-allocate: write directly to memory, skip cache → used with write-through.\\n2.6 Types of Misses (3C Model)\\nAll cache misses fall into one of three classes (Fig. 5.10):\\nCompulsory: first reference to a block.\\nCapacity: working set larger than cache capacity.\\nConflict: multiple blocks mapping to same set.\\nIncreasing block size reduces compulsory misses (spatial locality); increasing total capacity re\\xad\\nduces capacity misses; increasing associativity reduces conflict misses.\\n2.7 Quantitative Analysis: Average Memory Access Time\\nAMAT = thit + rmiss × tpenalty.\\n14'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15'}, page_content='Figure 8: An eight-block cache configured as direct mapped, two-way set associative, four-way set \\nassociative, and fully associative. (Fig. 5.15 in Patterson & Hennessy).\\nExample 1 – Single Level. Hit = 1 cycle, miss rate = 5 %, penalty = 50 cycles ⇒ AMAT = \\n1 + 0.05×50 = 3.5 cycles.\\nExample 2 – Two Levels. L1: 1 cycle, 5 % miss; L2: 10 cycles, 10 % of L1 misses; Memory: \\n100 cycles:\\nAMAT = 1 + 0.05(10 + 0.1 × 100) = 2.0 cycles.\\nExample 3 – Associativity Trade-off. Direct-mapped: hit = 1 cycle, miss rate = 5 %; \\n2-way: hit = 2 cycles, miss rate = 3 %; penalty = 50 cycles.\\nAMATDM = 1 + 0.05 × 50 = 3.5, AMAT2-way = 2 + 0.03 × 50 = 3.5.\\nHere associativity removes conflicts but increases hit time; overall effect can balance.\\n2.8 Pipeline Interaction: Instruction vs. Data Cache\\nIn pipelined CPUs, both instruction fetch (IF) and data memory (MEM) stages access memory \\neach cycle. To avoid conflicts, most designs split L1 into:\\n• an Instruction cache (I-cache) – read-only for fetching instructions,\\n• a Data cache (D-cache) – read/write for load-store data.\\nLower levels (L2/L3) are unified. Thus, IF and MEM can proceed simultaneously without con\\xad\\ntention.\\n15'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 15, 'page_label': '16'}, page_content='Figure 9: Illustrative only! Building a 4-way set associative cache.\\nAdditional Contents (non-examinable): Locality and Data Layout in High-Level \\nLanguages\\nCache principles apply to all programming languages through memory layout.\\nRow-major vs. Column-major order.\\n• Row-major (C, C++, Python/NumPy): consecutive elements of each row are contigu\\xad\\nous.\\n• Column-major (Fortran, MATLAB): consecutive elements of each column are contiguous.\\nTraversing memory in storage order improves spatial locality.\\nArrays of arrays. Python lists of lists store each row as a separate object, possibly far apart in \\nmemory. NumPy arrays store one contiguous buffer, allowing efficient cache use.\\nExample – Matrix Multiplication Order.\\nfor i in range(N):\\n    for j in range(N):\\n        for k in range(N):\\n            C[i][j] += A[i][k] * B[k][j]\\n16'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 16, 'page_label': '17'}, page_content='This version scans B by column → poor locality in row-major storage. Reordering:\\nfor i in range(N):\\n    for k in range(N):\\n        aik = A[i][k]\\n        for j in range(N):\\n            C[i][j] += aik * B[k][j]\\nnow scans B row-wise → better cache reuse.\\nKey ideas.\\n• Matching loop order to array layout improves spatial locality.\\n• Small contiguous “working sets” exploit temporal locality.\\n• Understanding these patterns helps design faster software even in high-level languages.\\nAdditional (non-examinable): Typical Parameters in Real Systems\\n Level  Typical size  Associativity  Hit ∼ cycles  Notes\\n L1 I/D  32 – 64 KiB  4 – 8  3 – 5  Split I/D, often VIPT\\n L2  256 KiB – 2 MiB  4 – 8  10 – 20  Unified\\n L3  8 – 64 MiB  8 – 16  30 – 50+  Shared in multicore\\n TLB L1  32 – 128 entries  4 – 8 ≈ 1  Separate I/D common\\n DRAM — —  50 – 200  Depends on freq/DDR gen\\nIllustrative only; not examinable.\\n2.9 Summary of Section 2\\n• Cache = small, fast SRAM memory that stores blocks of main memory.\\n• Each address divides into tag / index / offset.\\n• Mapping strategies:\\n– Direct-mapped – one line per set (simple, high conflict risk).\\n– Set-associative – multiple ways per set (balanced).\\n– Fully associative – any line (flexible, expensive).\\n• Each set physically stores a small table of entries (tag, valid, data). On access, all tags in that \\nset are compared in parallel.\\n• Replacement policies: LRU, Random, Pseudo-LRU.\\n• Write policies: write-through / write-back, write-allocate / no-write-allocate.\\n• AMAT quantitatively measures performance; associativity and block size affect both hit time \\nand miss rate.\\n• Split I/D caches prevent pipeline conflicts.\\n• (Non-examinable) Locality awareness in software leads to cache-efficient algorithms.\\n17'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18'}, page_content='Concept check (Section 2)\\n1. If we double block size, which 3C miss typically decreases first? Compulsory (spatial locality).\\n2. What parameter primarily reduces conflict misses? Higher associativity.\\n3. In a direct-mapped cache, what determines the line index? Block number mod #lines.\\nCommon pitfalls\\n• Do not mix hex nibbles with bit fields; rewrite addresses in binary when counting bits.\\n• Keep units straight: cycles vs ns; restate the frequency when converting.\\n• For set-associative caches, tags are per way in the selected set; compare all ways in parallel.\\nPrefetching (non-examinable). Hardware prefetchers fetch the next line (or detect short strides) \\nto reduce compulsory misses; software may issue prefetch hints. Prefetching helps when access pat\\xad\\nterns are predictable.\\nMulticore note (non-examinable). Real systems enforce coherence so cores see a consistent \\nview of memory. Our single-core cache model remains valid for understanding locality, mapping, \\nand AMAT; coherence adds additional traffic and latency not covered here.\\n3 Exercises: Caches\\n3.1 Purpose and Overview\\nThis section consolidates the principles of cache design through quantitative practice. The exercises \\nemphasize direct-mapped caches—because they are the simplest to analyze—and Average Memory \\nAccess Time (AMAT) evaluation under varying parameters. Worked examples are written step by \\nstep; the remaining exercises are intended for individual practice. A final subsection offers a few \\noptional, more advanced problems on associative caches and replacement policies.\\n3.2 Worked Example 3.1 — Direct-Mapped Cache Indexing and Miss Pattern\\nProblem. A processor uses a 32-bit address space and a 2 KB direct-mapped data cache with \\n32-byte blocks. The following byte addresses are accessed in order:\\n0x0000, 0x0020, 0x0040, 0x0060, 0x0000, 0x0020.\\nDetermine for each access whether it is a hit or a miss, and explain why.\\nStep 1 – Compute parameters.\\n2048 B cache\\n32 B per block = 64 blocks (cache lines) = 26.\\nTherefore:\\nOffset bits = log2(32) = 5, Index bits = log2(64) = 6, Tag bits = 32 − 6 − 5 = 21.\\nEach address can be divided as:\\ntag[31:11]⏞ ⏟⏟ ⏞\\n21 bits\\n| index[10:5]⏞ ⏟⏟ ⏞\\n6 bits\\n| block offset[4:0]⏞ ⏟⏟ ⏞\\n5 bits\\n.\\n18'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 18, 'page_label': '19'}, page_content='Step 2 – Determine block numbers. A block is 32 bytes, so each consecutive block starts 32 \\nbytes (0x20 in hexadecimal) after the previous one. We can find the block number by dividing the \\naddress by 32.\\n0x0000/32 = 0 ⇒ block number 0,\\n0x0020/32 = 1 ⇒ block number 1,\\n0x0040/32 = 2 ⇒ block number 2,\\n0x0060/32 = 3 ⇒ block number 3.\\nStep 3 – Compute cache line indices. Because the cache is direct-mapped, each block can only \\ngo in one line, found by:\\nCache line index = (block number) mod (number of lines).\\nThere are 64 lines, so mod64 selects the lower 6 bits of the block number.\\nBlock 0 ⇒ 0 mod 64 = line 0,\\nBlock 1 ⇒ 1 mod 64 = line 1,\\nBlock 2 ⇒ 2 mod 64 = line 2,\\nBlock 3 ⇒ 3 mod 64 = line 3.\\nTherefore, 0x0020 maps to cache line 1 because it belongs to block 1, and\\n1 mod 64 = 1.\\nEach successive block occupies the next cache line in order.\\nStep 4 – Trace the sequence.\\n• 0x0000: miss → fill line 0 (tag 0)\\n• 0x0020: miss → fill line 1 (tag 0)\\n• 0x0040: miss → fill line 2 (tag 0)\\n• 0x0060: miss → fill line 3 (tag 0)\\n• 0x0000: hit (tag 0 still in line 0)\\n• 0x0020: hit (tag 0 still in line 1)\\nStep 5 – Summary. The first access to each block causes a compulsory miss (first reference to \\nthat block). Subsequent accesses to the same block hit because the corresponding cache line still \\ncontains that data.\\n3.3 Worked Example 3.2 — Conflict Misses in a Direct-Mapped Cache\\nProblem. Same configuration (2 KB direct-mapped, 32 B blocks). Access sequence:\\n0x0000, 0x0800, 0x1000, 0x0000, 0x1000\\nStep 1 – Mapping conflict. Cache has 64 lines → index = 6 bits. Blocks differing by multiples \\nof 64 × 32 = 2048 B map to the same line.\\nStep 2 – Pattern. Addresses differ by 0x0800 = 2048 B → same index, different tags.\\n19'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 19, 'page_label': '20'}, page_content='Step 3 – Trace.\\n• 0x0000: miss (tag 0)\\n• 0x0800: miss (tag 1, replaces 0)\\n• 0x1000: miss (tag 2, replaces 1)\\n• 0x0000: miss again (tag 0 ≠ 2)\\n• 0x1000 (final access): hit (still resident from previous step).\\nConclusion. Each access evicts the previous one —pure conflict misses. Such pathologies mo\\xad\\ntivate set-associative caches or address interleaving in software.\\n3.4 Worked Example 3.3 — AMAT Under Different Parameters\\nProblem. Compare the effect of different cache parameters on performance.\\nCase A: L1 hit = 1 cycle, miss rate = 5 %, penalty = 50 cycles. Case B: same cache but miss \\nrate = 2 % (larger cache). Case C: same miss rate as A but penalty = 30 cycles (faster memory).\\nStep 1 – Formula.\\nAMAT = thit + rmiss × tpenalty.\\nStep 2 – Compute.\\nA: 1 + 0.05 × 50 = 3.5,\\nB: 1 + 0.02 × 50 = 2.0,\\nC: 1 + 0.05 × 30 = 2.5.\\nInterpretation. Reducing miss rate or penalty both improve AMAT, but their relative benefit \\ndepends on which term dominates. Hardware (memory speed) and architecture (cache size) thus \\ninteract directly in total CPI.\\n3.5 Worked Example 3.4 — Address Fields in a Direct-Mapped Cache\\nProblem. A 1 KiB direct-mapped cache with 16 B blocks receives address 0xABCD. Find the tag, \\nindex, and offset fields assuming 16-bit addresses.\\nParameters. Cache size = 210 B, block size = 24 B. Number of lines = 210/24 = 26. Hence: \\noffset = 4 bits, index = 6 bits, tag = 16 − 6 − 4 = 6 bits.\\nBinary breakdown. 0xABCD = 1010 1011 1100 11012.\\nTag[15 : 10] = 101010 (0x2A), Index[9 : 4] = 111100 (0x3C), Offset[3 : 0] = 1101 (0xD).\\nInterpretation. Index = 0x3C = 60 selects cache line 60, tag = 0x2A must match for a hit, and \\nthe byte is at offset 13 within its 16 B block.\\n20'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='3.6 Exercises\\nThe following problems build intuition for direct-mapped cache behavior and AMAT estimation. \\nWork them carefully by hand before using any simulator.\\n3.6. Basic Tag/Index/Offset Computation. 32-bit address, 4 KB direct-mapped cache, 16 B \\nblocks. Find tag, index, and offset sizes. Hint: 4096/16 = 256 lines → 8 index bits.\\n3.6. Address Tracing 1. Same cache; access sequence 0x00, 0x10, 0x20, 0x00, 0x30. Determine \\nhits/misses and classify the miss types. Hint: Each block = 16 B; draw a small table of line \\n↔ tag.\\n3.6. Address Tracing 2. 2 KB cache, 32 B blocks, direct-mapped. Accesses 0x0000, 0x0800, \\n0x1000, 0x1800. Predict the pattern. Hint: Stride = cache size → conflict every time.\\n3.6. Block Size Effect on AMAT. Hit = 1 cycle, miss rate = 8 % with 16 B blocks; 32 B \\nblocks reduce miss rate to 5 % but raise hit time to 2 cycles. Compute both AMATs. Hint:\\n1 + 0.08M vs. 2 + 0.05M with same penalty M.\\n3.6. Impact of Memory Speed. Hit = 1 cycle, miss rate = 4 %, penalty = 100 cycles. If faster \\nDRAM halves penalty, what is the percentage speed-up? Hint: Compare old/new AMAT.\\n3.6. Nested AMAT. L1: hit 1 cycle, miss 5 %; L2: hit 10 cycles, miss 10 % of L1 misses; Memory \\n= 100 cycles. Compute overall AMAT. Hint: 1 + 0.05(10 + 0.1 × 100).\\n3.6. Performance Scaling. For a CPU at 3 GHz, compute nanoseconds per access for AMAT = \\n2 cycles. Hint: 1 cycle = 1/3 ns.\\nOptional (Advanced): Associativity and Replacement Policies\\nThe following problems extend the analysis to associative caches and realistic policies. They are \\nrecommended for deeper understanding but are not examinable.\\nO3.6. 2-Way Set-Associative Mapping. 8 KB cache, 16 B blocks, 2-way associative. Compute \\ntag/index/offset bits and explain how associativity changes miss patterns compared to direct-\\nmapped.\\nO3.6. Replacement Policy Trace. 2-way cache, LRU replacement. Access sequence: A, B, A, \\nC, A, B. Which blocks are evicted at each step? Hint: Track recency of each set.\\nO3.6. Write-Back vs. Write-Through Timing. Suppose 25 % of accesses are writes; memory \\nwrite takes 40 cycles. Compare total memory traffic for write-through and write-back policies \\nassuming 5 % miss rate. Hint: Only dirty blocks cause writes in write-back.\\nO3.6. Effect of Associativity on AMAT. L1 direct-mapped: miss rate = 5 %; L1 2-way: miss \\nrate = 3 %, hit time +1 cycle. Compute which is faster given 50-cycle penalty. Hint: Compare \\nAMATs quantitatively.\\n3.7 Concluding Remarks\\nThese problems illustrate how cache performance arises from quantitative interactions among block \\nsize, cache size, associativity, and memory speed. Always begin with a clear understanding of the \\naddress breakdown and the hierarchy’  s timing model. Mastery of direct-mapped caches forms the \\nfoundation for the virtual-memory mechanisms studied next.\\n21'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='4 Virtual Memory and Address Translation\\nBefore we start: the tiny translator in the CPU\\nWhen a program uses memory, it asks for a location by giving a number. This number is the \\nprogram’s own way of naming a location; it is not yet a real slot in the computer’s memory chips.\\nInside the processor there is a small hardware translator that turns the program’s number into \\na real location in memory. This translator is called the Memory Management Unit (MMU).\\nAt a high level:\\n• The program provides a number we will call a virtual address.\\n• The MMU converts it into a physical address, which is where the data actually live in DRAM.\\n• To know how to convert, the MMU consults a table prepared by the operating system (we \\nwill define this table in the next pages).\\n• To go fast, the MMU also keeps a few recent conversions in a tiny hardware cache; we will \\ngive this cache a name later in this section.\\nAfter this brief overview, we will now define these terms carefully (virtual address, physical \\naddress, pages, the table, and the small cache) and work through step-by-step examples.\\n4.1 Motivation\\nUp to this point, we have worked with physical addresses—the real locations of data in memory. \\nModern computer systems introduce an additional layer called virtual memory, which allows each \\nrunning program to behave as though it has a large, private, and contiguous memory space.\\nThis layer serves three essential purposes:\\n1. Isolation: each process runs in its own protected address space.\\n2. Protection: one program cannot accidentally overwrite another’s data.\\n3. Flexibility: programs can use large contiguous virtual spaces even when physical memory is \\nfragmented or partly stored on disk.\\nVirtual memory is implemented through cooperation between the processor hardware and a \\nsoftware component of the system called the operating system (OS). The hardware part performs \\nfast address translation; the OS manages which parts of each program are currently loaded in \\nmemory.\\nCPU (virtual address) → MMU (hardware translation) → Physical Memory\\n4.2 Pages and Frames\\nBoth virtual and physical memories are divided into fixed-size blocks:\\n• A page is a contiguous block of virtual memory.\\n• A frame (or physical page) is an equally sized block in physical memory.\\nCommon page sizes are 4 KB, 8 KB, or 2 MB. Each virtual address is divided into two parts:\\nVirtual Page Number (VPN)⏞ ⏟⏟ ⏞\\nhigh bits\\n| Page offset⏞ ⏟⏟ ⏞\\nlow bits\\n.\\n22'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 22, 'page_label': '23'}, page_content='Figure 10: The concept of “Virtual Memory”. In virtual memory, blocks of memory (called pages) \\nare mapped to physical addresses. Both virtual and physical memory are split into pages (1 virtual \\npage maps to 1 physical page). Processors generate virtual addresses, while memory is accessed \\nusing physical addresses. A hardware component called the MMU, or Memory Management Unit, \\nis in charge of doing this translation (Fig 5.24 Patterson & Hennessy).\\nIf the virtual address has m bits and each page has 2p bytes, then:\\nVPN bits = m − p, Offset bits = p.\\nThe offset selects a byte within a page (it is copied unchanged into the physical address). The \\nVPN is used to index a data structure called the page table, which holds the corresponding physical \\nframe number (PFN).\\n23'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 23, 'page_label': '24'}, page_content='Figure 11: Translating a virtual address to a physical address. (Fig 5.25 Patterson & Hennessy)\\nExtra contents:\\nOnce we define a page as a fixed-size unit of virtual memory (for example 4 KiB), several design \\nchoices arise:\\n• Page size. Pages should be large enough that the long access time to secondary storage \\n(for example, disk) is amortized over a reasonably large transfer. Typical page sizes today \\nrange from 4 KiB to 16 KiB. New desktop and server systems are moving toward 32 KiB \\nor even 64 KiB pages, while embedded systems often use smaller pages such as 1 KiB to \\nsave space.\\n• Reducing page-fault rate. It is desirable to minimize page faults because they involve \\nextremely high latency. One effective organization is to allow fully associative placement of \\npages in physical memory—any virtual page can be placed in any physical frame—so that \\nno page is forced out due to simple placement conflicts.\\n• Handling page faults. Because servicing a page fault already involves a very slow disk \\naccess, it is efficient to let software handle page faults and replacement decisions. The \\noverhead of software management is negligible compared to the disk latency, and it allows \\nthe use of sophisticated algorithms that can further reduce the miss rate.\\n• Write policy. A write-through policy is impractical for virtual memory, since writing to \\ndisk on every store would be far too slow. Therefore, virtual memory systems always use \\na write-back policy: modified pages are marked as “dirty” and written to disk only when \\nthey are replaced.\\n24'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 24, 'page_label': '25'}, page_content='Figure 12: The Page Table. (Fig 5.26 Patterson & Hennessy).\\nWorked Example 4.1 – Basic Translation\\nConsider a 16-bit virtual address and 1 KB pages (210 bytes). Then:\\nVPN bits = 6, Offset bits = 10.\\nSuppose that virtual page 5 maps to physical frame 12. For address 0x15A3:\\n0x15A3 = 000101⏞ ⏟⏟ ⏞\\nVPN=5\\n10100011⏞ ⏟⏟ ⏞\\noffset=0xA3\\n.\\nReplacing the VPN with the PFN:\\nPA = PFN ||offset = 00001100 ||10100011 = 0xCA3.\\nThus, virtual address 0x15A3 corresponds to physical address 0xCA3.\\n4.3 The Page Table\\nThe page table is a data structure maintained by the operating system but used by hardware during \\nmemory access. Each process has its own page table.\\nEach Page Table Entry (PTE) contains:\\n25'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26'}, page_content='• The physical frame number (PFN),\\n• A valid bit (1 if the page is currently loaded in memory),\\n• Protection bits (read/write/execute),\\n• Optional flags such as dirty and reference bits for the OS.\\nWhen a program accesses memory:\\n1. The Memory Management Unit(MMU) extracts the virtual page number.\\n2. It looks up the corresponding page table entry.\\n3. If valid = 1, the page is in memory and translation succeeds.\\n4. If valid = 0, a page fault occurs, and the OS loads the page from disk.\\nConnection to caches. The page table plays a similar role to a cache directory: both map logical \\naddresses (VPNs or block numbers) to physical locations.\\nWorked Example 4.2 – Page Table Lookup\\nAssume a 32-bit virtual address, 4 KB pages (212 bytes). VPN = upper 20 bits, offset = lower 12 \\nbits. If VPN = 0x00020 maps to PFN = 0x00007, then:\\nVA = 0x00020ABC → PA = 0x00007ABC.\\nOnly the upper bits change; the offset is preserved.\\nExtra:\\nA single-level page table can be very large. For example, a 64-bit address space with 4 KB pages \\nwould require 252 entries. To manage this, systems use multi-level page tables.\\nEach level acts like an index into a smaller table describing a portion of the address space. This \\nstructure drastically reduces the total size, because only the levels corresponding to used portions \\nof memory need to exist.\\n4.4 The Translation Lookaside Buffer (TLB)\\nAccessing the page table for every memory reference would be far too slow. To avoid this, the MMU \\nincludes a small, very fast cache of recent translations called the Translation Lookaside Buffer \\n(TLB).\\nEach TLB entry stores:\\n• The virtual page number (as the tag),\\n• The corresponding physical frame number,\\n• Access permissions and valid bit.\\nOperation.\\n1. On every memory access, the MMU checks the TLB for the VPN.\\n2. On a TLB hit, the PFN is returned immediately.\\n3. On a TLB miss, the hardware or OS must consult the page table, insert the translation into \\nthe TLB, and retry the access.\\n26'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 26, 'page_label': '27'}, page_content='Figure 13: Virtual addresses are larger than physical addresses. That means we cannot store all \\nvirtual pages in physical pages. (Fig 5.27 Patterson & Hennessy).\\nWorked Example 4.3 – Effective Memory Access Time (EMAT)\\nAssume:\\n• TLB hit time = 1 ns,\\n• TLB hit rate = 99%,\\n• Memory access time = 100 ns.\\nEMAT = (0.99)(1 + 100) + (0.01)(1 + 200) = 102 ns.\\nEven with a small miss rate, the extra translation time is noticeable.\\n4.5 Page Faults and the Role of the Operating System\\nWhen the page table indicates that a page is not in memory (valid = 0), the MMU raises a page \\nfault. At this point, the operating system must intervene.\\nAt a high level what happens is the following:\\n1. The processor temporarily stops the running program.\\n2. The Operating System locates the missing page on disk.\\n3. It copies the page into a free physical frame in RAM.\\n27'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 27, 'page_label': '28'}, page_content='Figure 14: The TLB. (Fig 5.28 Patterson & Hennessy).\\n4. The OS updates the page table entry to make it valid.\\n5. The processor restarts the instruction that caused the fault.\\nFrom the program’s point of view, this all happens automatically, but a page fault is millions \\nof times slower than a normal memory access. This is why programs that reuse recently accessed \\ndata (good locality) perform much better.\\n4.6 Interaction with the Cache Hierarchy\\nCaches and virtual memory operate together:\\n• The MMU translates virtual to physical addresses before cache access.\\n• Caches store data using physical addresses to avoid confusion between processes.\\n• The TLB itself behaves as a tiny associative cache for page table entries.\\nIn pipelined processors, address translation and cache lookup are performed in parallel to avoid \\nslowing down memory access.\\n4.7 Summary of Section 4\\n• Virtual memory maps each process’s virtual addresses to physical memory frames.\\n• The MMU performs this translation automatically on every memory access.\\n• The page table holds the mappings; the TLB caches the most recent ones.\\n28'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 28, 'page_label': '29'}, page_content='Figure 15: Virtual Memory and Cache working together to fetch a memory block. (Fig 5.29 Pat\\xad\\nterson & Hennessy).\\n• Page faults occur when data must be fetched from disk; the OS manages this.\\n• Locality is just as important here as in caching: good locality means fewer TLB misses and \\npage faults.\\nConcept check (Section 4)\\n1. Which bits are preserved from VA to PA? Page offset.\\n2. What does a TLB store? Mappings from VPN to PFN with permissions.\\n3. What triggers a page fault? PTE valid bit = 0 for the referenced VPN.\\n29'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30'}, page_content='5 Assembly Programming and the Integrated Memory View\\n5.1 Why Addressing Modes Matter\\nAt the hardware level, every instruction must specify where its operands come from and where the \\nresult goes. The patterns by which instructions access data are called addressing modes.\\nThese modes determine:\\n• whether the operand comes from a register or from memory,\\n• how the memory address is computed,\\n• how many memory accesses are required, and therefore\\n• how much time the instruction will take when executed on a pipelined processor with caches.\\nEfficient use of addressing modes is central to writing code that respects the memory hierarchy. \\nUnderstanding them also helps interpret compiler output and reason about memory locality.\\n5.2 MIPS Addressing Modes\\nThe MIPS architecture has a simple and regular design. Each instruction fits in 32 bits, and memory \\nis accessed only by explicit load (‘lw‘, ‘lh‘, ‘lb‘) and store (‘sw‘, ‘sh‘, ‘sb‘) operations.\\n1. Immediate addressing. The operand is a constant encoded directly in the instruction.\\naddi $t0, $t1, 4     # $t0 = $t1 + 4\\nNo memory access occurs here: the constant 4 is part of the instruction itself.\\n2. Register addressing. Both operands come from registers.\\nadd $s0, $s1, $s2   # $s0 = $s1 + $s2\\nAgain, this uses only the register file; no data cache access.\\n3. Base (displacement) addressing. A constant offset is added to a register to form a memory \\naddress.\\nlw $t0, 8($s1)       # load word from address ($s1 + 8) into $t0\\nsw $t0, 0($s2)       # store word from $t0 to address ($s2 + 0)\\nThis is the standard way to access arrays or structure fields in MIPS. If $s1 holds the base address \\nof an array, the offset selects an element.\\n4. PC-relative addressing.  Used for branches and jumps: the new program counter (PC) is \\ncomputed as the current PC plus a signed offset.\\nbeq $t0, $t1, target   # if equal, PC = PC + offset to target\\nThis mode improves code locality: short branches typically target nearby addresses, which remain \\nin the instruction cache.\\n5. Pseudo-direct addressing.  Used by the ‘j‘ and ‘jal‘ instructions. The target address is \\nformed by combining high bits of the current PC with a 26-bit field in the instruction.\\njal 0x00400000          # jump and link to fixed address\\n30'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31'}, page_content='5.3 x86_64 Addressing Modes (for comparison)\\nWhile MIPS keeps addressing simple, x86_64 provides richer options. You are not required to \\nmemorize them for the exam, but learning their structure helps when reading compiler output.\\nIn AT&T syntax (used by GNU assemblers), the general form of a memory operand is:\\ndisp(base, index, scale)\\nwhich corresponds to the effective address:\\nEA = disp + base + index × scale.\\nExamples:\\nmovq  $8(%rax), %rbx        # base + displacement\\nmovq  (%rax,%rcx,4), %rdx   # base + index * scale\\nmovq  $16(%rbp,%rcx,8), %rax\\n• ‘disp‘ is an immediate displacement (can be zero).\\n• ‘base‘ is a general-purpose register.\\n• ‘index‘ is an optional register multiplied by a scale factor (1, 2, 4, or 8).\\nThese allow direct addressing of array elements or structure fields in memory. For instance, if ‘\\nConnection with MIPS. The x86_64 effective address formula combines the base and index \\ncomputation that MIPS expresses using arithmetic instructions before a load. In both architectures, \\nthe final computed address passes through the same cache hierarchy and translation stages discussed \\nearlier.\\n5.4 Efficiency and the Memory Hierarchy\\nFrom the memory-system perspective, instructions differ in cost:\\n• Register and immediate addressing: no memory access; fastest.\\n• Base/displacement addressing: one data cache access; slower.\\n• Indirect addressing (in x86): may cause multiple dependent memory accesses.\\nPrograms that repeatedly use data already in registers or in nearby memory exhibit temporal \\nand spatial locality. The cache system can then deliver data quickly, and few TLB or page-table \\nlookups are needed.\\nExample: Loop locality.\\nLoop:\\n    lw  $t0, 0($s0)\\n    addi $s0, $s0, 4\\n    add  $t1, $t1, $t0\\n    bne  $s0, $s2, Loop\\nIf $s0 and $s2 delimit consecutive memory locations, accesses are sequential, benefiting from spatial \\nlocality. If the loop instead followed pointers scattered across memory, cache and TLB misses would \\nincrease dramatically.\\n31'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32'}, page_content='6 Exercises: Virtual Memory\\nWorked Example 6.1 – Virtual to Physical Address Translation\\nA system uses 16-bit virtual addresses and 1 KB pages ( 210 bytes). Physical memory contains 8 \\nframes.\\n1. Split the virtual address into its fields:\\nVPN (6 bits) | Offset (10 bits).\\n2. Suppose the page table contains:\\n VPN  Valid  PFN\\n 0  1  3\\n 1  0  –\\n 2  1  6\\n 3  1  4\\n 4  0  –\\n 5  1  2\\n3. Translate virtual address 0x15A3 step by step:\\n(a) Page size = 1 KB = 210. Offset = low 10 bits = 0x0A3. VPN = bits [15:10] = 5.\\n(b) In the page table: VPN 5 → PFN 2 (valid = 1).\\n(c) Replace the VPN with the PFN, keep the same offset:\\nPA = (PFN ≪ 10) + offset = (2 × 1024) + 0x0A3 = 0x08A3.\\nResult: Virtual address 0x15A3 maps to physical 0x08A3.\\nWorked Example 6.2 – TLB Lookup\\nAssume the same address space (16 bits, 1 KB pages). A 4-entry TLB stores recent translations:\\n Entry  VPN  PFN  Valid\\n 0  0  3  1\\n 1  2  6  1\\n 2  5  2  1\\n 3  7  4  1\\nTranslate virtual address 0x15A3:\\n1. Extract VPN = 5.\\n2. Search the TLB: entry 2 matches (hit).\\n3. PFN = 2 → Physical address = 0x08A3.\\nIf VPN 5 were not found, the MMU would check the page table in memory (a miss).\\n32'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 32, 'page_label': '33'}, page_content='Worked Example 6.3 – Effective Memory Access Time (EMAT)\\nParameters:\\n• Memory access = 100 ns\\n• TLB hit time = 1 ns\\n• TLB hit rate = 95 %\\n• Page-table lookup = one extra memory access on miss\\nEMAT = (0.95)(1 + 100) + (0.05)(1 + 200) = 106 ns.\\nEven a small miss rate noticeably slows access.\\n6.1 Exercises\\nEach question below asks for a concrete operation: perform the translation, indicate hit/miss, or \\ncompute a value.\\n6.1. Address Breakdown. 32-bit address, 4 KB pages. (a) How many bits are used for the page \\noffset? (b) How many bits remain for the VPN?\\n6.1. Page Table Translation. 16-bit virtual addresses, 1 KB pages. Given the page table:\\n VPN  Valid  PFN\\n 0  1  5\\n 1  1  3\\n 2  0  –\\n 3  1  1\\nTranslate the following virtual addresses or state that a page fault occurs:\\n• (a) 0x03A2\\n• (b) 0x0D10\\n• (c) 0x07F5\\n6.1. TLB Lookup. The TLB below contains recent entries. Determine for each virtual address \\nwhether the lookup hits or misses, and if it hits, give the physical address.\\n Entry  VPN  PFN  Valid\\n 0  0x04  0x08  1\\n 1  0x07  0x02  1\\n 2  0x0A  0x05  1\\n 3  0x0C  0x06  1\\nVirtual addresses (hex): (a) 0x0A14�(b) 0x0710�(c) 0x0408�(d) 0x0D30\\nAssume page size = 1 KB.\\n6.1. Compute EMAT. Memory access = 120 ns, TLB access = 2 ns, TLB hit rate = 97%. On a \\nTLB miss, the page-table lookup costs one extra memory access. (a) Write the exact EMAT \\nformula you are using. (b) Compute EMAT. (c) If hit rate improves to 98.5%, what is the \\npercent speed-up?\\n33'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34'}, page_content='6.1. Page Fault Recognition. Given a page table with a valid bit array [1, 1, 0, 1, 0, 1], indicate \\nfor VPN = 2 whether access succeeds or triggers a page fault. What component (hardware \\nor OS) handles it?\\n6.1. Physical Memory Size. Physical address = 18 bits, page size = 4 KB. (a) How many \\nframes exist? (b) What is the total physical memory capacity?\\n(Optional) Hierarchical Lookup. A 32-bit virtual address is split into: 10 bits top-level index, \\n10 bits second-level index, 12 bits offset. What portion of the virtual address space does each \\ntop-level entry describe?\\n7 Integration and Summary\\n7.1 The Complete Memory Hierarchy\\nAll modern systems organize memory in levels to balance speed and cost:\\nCPU registers → L1 cache → L2/L3 cache → Main memory (DRAM) → Disk or SSD (storage).\\nEach level:\\n• stores copies of data from lower levels,\\n• is smaller and faster than the next,\\n• exploits temporal and spatial locality.\\n7.2 Address Flow\\nWhen the CPU executes a load or store:\\n1. The instruction generates a virtual address.\\n2. The MMU translates it to a physical address (typically using a TLB).\\n3. The cache checks whether the block containing that physical address is present (hit or miss).\\n4. On a hit, data are returned; on a miss, the block is fetched from main memory.\\n5. If the page is not resident (valid bit = 0), a page fault occurs; the OS loads the page and \\nupdates the page table before the access can proceed.\\nWorked Example 7.1 – End-to-End Reference\\nA MIPS processor issues a load from virtual address 0x00402A10. Assume 4 KB pages.\\n1. Split VA into VPN and offset: VPN = 0x00402, offset = 0x0A10.\\n2. TLB lookup for VPN 0x00402 hits with PFN 0x0017.\\n3. Physical address = 0x0017A10.\\n4. The data cache probes the set indexed by this physical address; if the tag matches and valid \\n= 1, it is a cache hit and the load completes.\\n5. If the cache misses, the block containing 0x0017A10 is fetched from DRAM into the cache; \\nthe load then completes.\\n34'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35'}, page_content='7.3 Design Trade-offs\\n• Cache parameters: block size, total size, associativity, and replacement policy influence hit \\nrate and hit time (and therefore AMAT).\\n• Write policies: write-through vs. write-back and write-allocate vs. no-write-allocate affect \\nbandwidth and latency.\\n• Virtual memory parameters: page size influences TLB coverage and page-fault frequency; \\nTLB size/associativity affects hit rate and lookup time.\\n7.4 Summary of Section 7\\n• Each level acts as a cache for the next level down.\\n• Virtual memory extends the hierarchy to secondary storage and isolates processes.\\n• Address translation (MMU and TLB) precedes cache lookup in typical designs.\\n• Good locality improves both cache and TLB hit rates.\\n• The same core ideas recur at all levels: placement (mapping), identification (tags), replace\\xad\\nment, and write/consistency policies.\\nA Reference Tables\\nA.1 MIPS vs x86_64 Addressing Summary\\n Concept  MIPS  x86_64 (AT&T)\\n Memory operand form  offset($base)  disp(%base,%index,scale)\\n Example  lw $t0, 8($s1)  movq 8(%rsi), %rax\\n Base + displacement  Yes  Yes\\n Index register  Computed in ALU  Built-in (%index)\\n Scale factor  1 only  1, 2, 4, 8\\n Load/store discipline  Load/store architecture  Memory operands in many ops\\n Typical array access  lw $t0, 4($s1)  movl (%rdi,%rcx,4), %eax\\nA.2 Glossary\\nAMAT Average Memory Access Time; hit time + miss rate × miss penalty.\\nBlock (cache line) Fixed-size unit transferred between cache and memory.\\nFrame Fixed-size physical memory block for a virtual page.\\nLocality Temporal: reuse over time; Spatial: reuse of nearby addresses.\\nMMU Memory Management Unit; performs virtual-to-physical translation.\\nPage Fixed-size unit of virtual memory; size commonly 4 KB.\\nPTE Page Table Entry; maps VPN to PFN and stores permissions and valid/dirty bits.\\nTLB Translation Lookaside Buffer; small associative cache of recent VPN to PFN mappings.\\nVPN/PFN Virtual Page Number / Physical Frame Number.\\n35'), Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 35, 'page_label': '36'}, page_content='Answer key (final numbers only; not examinable)\\nSection 3 (Caches)\\n3.1 Tag/index/offset for 4 KB DM, 16 B blocks, 32-bit: offset 4, index 8, tag 20.\\n3.2 With sequence 0x0000, 0x0800, 0x1000, 0x0000, 0x1000: miss, miss, miss, miss, hit.\\n3.3 2-way, parameters: sets = 4096/(16 · 2) = 128 ⇒ offset 4, index 7, tag 21; trace as written.\\n3.4 1 KiB DM, 16 B blocks, 16-bit: offset 4, index 6, tag 6; 0xABCD ⇒ tag 0x2A, index 0x3C, offset \\n0xD.\\nAMAT example: 1 + 0.05 · 50 = 3.5 cycles; improved 1 + 0.02 · 50 = 2.0 cycles.\\nSection 6 (Virtual Memory)\\n6.1 VA 0x15A3 ⇒ PA 0x08A3.\\n6.2 TLB hit for VPN 5 ⇒ PFN 2 ⇒ PA 0x08A3.\\n6.3 EMAT = (0.95)(1 + 100) + (0.05)(1 + 200) = 106 ns.\\n6.4 EMAT formula: EMAT = h · (tTLB + tmem) + (1 − h) · (tTLB + 2tmem).\\nNumbers: h = 0.97, tTLB = 2 ns, tmem = 120 ns ⇒ EMAT = 0.97·122+0.03·242 = 125.6 ns. Speed-\\nup to h = 0.985: new EMAT = 0.985·122+0.015·242 = 123.8 ns; speed-up ≈ (125.6−123.8)/125.6 ≈\\n1.4%. Frames for 18-bit PA, 4 KB pages: 218/212 = 26 = 64 frames; capacity = 64 ·4 KB = 256 KB.\\n36')]\n"
     ]
    }
   ],
   "source": [
    "## Reading a PDF File now\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r'C:\\Users\\Karan\\Desktop\\agentic-ai-langchain-langgraph\\LangChain\\1. LangChain\\DataIngestion\\computer_systems_lect.pdf')\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b65b8",
   "metadata": {},
   "source": [
    "### How to recursively split text by characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4578348",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678e4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed6b7e",
   "metadata": {},
   "source": [
    "### text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "### final_documents = text_splitter.create_documents(docs)\n",
    "### YOU WILL GET A ERROR HERE BECAUSE THE FUNCTION WE HAVE USED IS SAYING 'CREATE DOCUMENT' BUT DOCUMENTS ARE ALREADY CREATED  BEFORE. SO WE WILL USE 'SPLIT DOCUMENT' FUNCTION INSTEAD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e92c9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='Memory Hierarchy: Caches and Virtual Memory\\n David Castro-Perez\\nNovember 10, 2025\\nContents\\n1 The Memory Hierarchy in Context 3\\n1.1 Definition and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 The Principle of Locality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.3 Structure of the Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='1.4 Quantitative View: Average Memory Access Time (AMAT) . . . . . . . . . . . . . . 6\\n1.5 The Impact on CPU Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.6 Trade-offs in the Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2 Caches: Organization, Operation, and Performance 8'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='2.1 From Concept to Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Cache Structure and Address Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3 Mapping Strategies: Where Does Each Block Go? . . . . . . . . . . . . . . . . . . . 10\\n2.4 Replacement Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5 Write Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='2.6 Types of Misses (3C Model) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.7 Quantitative Analysis: Average Memory Access Time. . . . . . . . . . . . . . . . . . 14\\n2.8 Pipeline Interaction: Instruction vs. Data Cache . . . . . . . . . . . . . . . . . . . . 15\\n2.9 Summary of Section 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3 Exercises: Caches 18'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='3 Exercises: Caches 18\\n3.1 Purpose and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.2 Worked Example 3.1 — Direct-Mapped Cache Indexing and Miss Pattern . . . . . . 18\\n3.3 Worked Example 3.2 — Conflict Misses in a Direct-Mapped Cache . . . . . . . . . . 19\\n3.4 Worked Example 3.3 — AMAT Under Different Parameters . . . . . . . . . . . . . . 20\\n3.5 Worked Example 3.4 — Address Fields in a Direct-Mapped Cache . . . . . . . . . . 20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='3.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4 Virtual Memory and Address Translation 22\\n4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4.2 Pages and Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}, page_content='4.3 The Page Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4.4 The Translation Lookaside Buffer (TLB) . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n4.5 Page Faults and the Role of the Operating System . . . . . . . . . . . . . . . . . . . 27\\n4.6 Interaction with the Cache Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.7 Summary of Section 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n1'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 1, 'page_label': '2'}, page_content='5 Assembly Programming and the Integrated Memory View 30\\n5.1 Why Addressing Modes Matter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n5.2 MIPS Addressing Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n5.3 x86_64 Addressing Modes (for comparison) . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.4 Efficiency and the Memory Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n6 Exercises: Virtual Memory 32'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 1, 'page_label': '2'}, page_content='6 Exercises: Virtual Memory 32\\n6.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n7 Integration and Summary 34\\n7.1 The Complete Memory Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.2 Address Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.3 Design Trade-offs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 1, 'page_label': '2'}, page_content='7.4 Summary of Section 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\nA Reference Tables 35\\nA.1 MIPS vs x86_64 Addressing Summary . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\nA.2 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The basic structure of a memory hierarchy. By implementing the memory system \\nas a hierarchy, the user has the illusion of a memory that is as large as the largest level of the \\nhierarchy, but can be accessed as if it were all built from the fastest memory. (Fig 5.1 in [Patterson \\n& Hennessy])\\nBased on (must-read): Patterson & Hennessy, Computer Organization and Design, MIPS \\nEdition, Chapter 5: “Large and Fast: Exploiting Memory Hierarchy”.\\n1 The Memory Hierarchy in Context'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3'}, page_content='1 The Memory Hierarchy in Context\\n1.1 Definition and Motivation\\nA memory hierarchy is a structure that uses multiple levels of memory. As the distance from the \\nprocessor increases, the size of each level increases, the access time also increases, and the cost per \\nbit decreases. This arrangement allows the computer to appear as though it has both large capacity \\nand high speed.\\nThe performance gap. Since the 1980s, processor clock rates have improved by several orders'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3'}, page_content='of magnitude, while DRAM latency has improved only modestly. A modern CPU core can exe\\xad\\ncute billions of operations per second, but each main-memory access may take hundreds of cycles. \\nWithout some intermediate storage, the processor would spend most of its time waiting for data.\\nAn intuitive analogy. Imagine a chef (the CPU) preparing meals in a kitchen. If every ingredient \\nwere stored in a warehouse miles away (the main memory), the chef would spend most of the day'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3'}, page_content='walking back and forth rather than cooking. To stay productive, the chef keeps the most commonly \\nused ingredients within arm’s reach (registers and caches), less frequently used items in a nearby \\npantry (main memory), and rare ingredients in the distant warehouse (disk or SSD). The closer the \\nstorage, the faster it is-but the less it can hold.\\nWhy a hierarchy works. Even though only a small fraction of the total data fits in the top'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3'}, page_content='levels, programs often reuse the same data and instructions repeatedly. If those active portions can \\n3'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 3, 'page_label': '4'}, page_content='Figure 2: Every pair of levels in the memory hierarchy can be thought of as having an \\nupper and lower level. Within each level, the unit of information that is present or not is called \\na block or a line. Usually we transfer an entire block when we copy something between levels. (Fig \\n5.2 in [Patterson & Hennessy])\\nstay near the processor, the system will behave as if all memory were fast.\\n1.2 The Principle of Locality'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 3, 'page_label': '4'}, page_content='1.2 The Principle of Locality\\nThe idea that makes the hierarchy work is the principle of locality. Empirically, programs do not \\naccess memory randomly; instead, they tend to use the same data and nearby data for extended \\nperiods.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 4, 'page_label': '5'}, page_content='Figure 3: This diagram shows the structure of a memory hierarchy: as the distance from the \\nprocessor increases, so does the size.This structure, with the appropriate operating mechanisms, \\nallows the processor to have an access time that is determined primarily by level 1 of the hierarchy \\nand yet have a memory as large as level n. Maintaining this illusion is the subject of Week 6 lectures \\n(Fig 5.3 in [Patterson & Hennessy])'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 4, 'page_label': '5'}, page_content='(Fig 5.3 in [Patterson & Hennessy])\\nTemporal locality If a location is referenced at one time, it is likely to be referenced again soon. \\nTypical examples include variables reused within a loop, recently called procedures, or ele\\xad\\nments of a stack frame that are touched repeatedly.\\nSpatial locality If a location is referenced, nearby locations are likely to be referenced soon. For \\ninstance, instructions are fetched sequentially, and arrays are traversed element by element.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 4, 'page_label': '5'}, page_content='Both forms of locality imply that recently and nearby used information is far more valuable than \\ndistant information. This is the key reason that small, fast memories can dramatically accelerate \\ncomputation.\\n1.3 Structure of the Hierarchy\\nEach level of the hierarchy acts as a cache for the level below it. The processor always interacts \\nwith the highest level (registers and caches), and only if data is missing does it access the next level.\\nLevels.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 4, 'page_label': '5'}, page_content='Levels.\\n• Registers: a few dozen to a few hundred bytes, accessed every clock cycle.\\n• Caches: small SRAM memories (tens of kilobytes to a few megabytes) that automatically \\nkeep frequently used data close to the CPU.\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6'}, page_content='• Main memory (DRAM): gigabytes of capacity, slower but inexpensive per bit.\\n• Secondary storage (SSD, HDD): terabytes of capacity, but millions of times slower than \\nregisters.\\nEach level exploits locality by storing blocks of data that are likely to be reused soon.\\nBlocks (or cache lines). Information moves between adjacent levels in fixed-size chunks called \\nblocks (or cache lines). A block is typically 16 – 128 bytes. When the CPU requests one byte or'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6'}, page_content='word, the entire block containing it is fetched into the higher level. If the program later accesses \\nanother word from that same block, the request will be satisfied quickly—this is spatial locality in \\naction.\\nHits and misses. If the requested data is already present in the current level, we have a hit; \\notherwise, we have a miss. The hit rate is the fraction of accesses served by that level, usually 90 \\n– 99 %. The miss penalty is the time required to fetch the data from the next level.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6'}, page_content='Control of data movement. Movement between levels is automatic and invisible to the pro\\xad\\ngrammer. Hardware caches manage blocks using address tags; operating systems manage pages \\nusing virtual-memory tables. In both cases, recently used information is retained while less useful \\ndata is displaced.\\n1.4 Quantitative View: Average Memory Access Time (AMAT)\\nTo evaluate how effective a hierarchy is, architects use the Average Memory Access Time \\n(AMAT):\\nAMAT = Hit time + Miss rate × Miss penalty.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6'}, page_content='AMAT = Hit time + Miss rate × Miss penalty.\\n• Hit time — the time to access data on a hit (includes tag check).\\n• Miss rate — the fraction of accesses that miss in this level.\\n• Miss penalty — the extra time to retrieve the block from the next level.\\nFor multi-level hierarchies, AMAT is computed recursively: the average access time of one level \\nbecomes the miss penalty of the level above.\\nWorked Example 1.1 — Quantifying the Effect of a Cache'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6'}, page_content='Suppose main memory access takes 100 ns. We add an L1 cache that can serve a hit in 2 ns and \\nhas a 95 % hit rate. Then:\\nAMAT = 2 + 0.05 × 100 = 7 ns.\\nThe cache reduces average access time by a factor of about 14.\\nTwo-level AMAT derivation. Let L1 have hit time t1 and miss rate m1. Let L2 have hit time \\nt2 and miss rate m2 (fraction of L1 misses), and main memory time tM.\\nAMAT = t1 + m1\\n(︂\\nt2 + m2 · tM\\n)︂\\n.\\nExample: t1 = 1, m1 = 0.05, t2 = 10, m2 = 0.10, tM = 100 ⇒ AMAT = 2.0 cycles.\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7'}, page_content='Worked Example 1.2 — Adding a Second Level\\nNow add an L2 cache that handles 90 % of the L1 misses in 10 ns. Only 10 % of L1 misses go to \\nmain memory (100 ns):\\nAMAT = 2 + 0.05(10 + 0.10 × 100) = 2 + 0.05 × 20 = 3 ns.\\nAdding L2 more than halves the average access time again.\\n1.5 The Impact on CPU Performance\\nEvery cache miss introduces extra cycles in the processor pipeline. If each instruction on average \\nperforms one memory access, we can express the total cycles per instruction (CPI) as:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7'}, page_content='CPItotal = CPIbase + (misses/instruction) × miss penalty.\\nExample. Base CPI = 1, L1 miss rate = 2 %, miss penalty = 50 cycles:\\nCPItotal = 1 + 0.02 × 50 = 2.\\nEven a tiny miss rate doubles total execution time. This is why architectural effort (multi-level \\ncaches, prefetching) and software effort (data locality) are both critical.\\n1.6 Trade-offs in the Hierarchy\\nEach level balances five competing factors:\\n• Latency: how fast each access is.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7'}, page_content='• Latency: how fast each access is.\\n• Bandwidth: how much data per second can be transferred.\\n• Capacity: total storage size.\\n• Cost per bit: economic efficiency of the technology.\\n• Complexity: hardware or software management overhead.\\nFast memories (SRAM) are small and expensive; slow memories (DRAM, SSD) are large and \\ncheap. By combining them, the system behaves like a single large, fast, affordable memory —\\nprovided locality holds.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7'}, page_content=\"provided locality holds.\\nAdditional Contents (non-examinable): Why Programmers Should Care\\nThis short section is not examinable. It shows the practical relevance of locality for anyone writing \\nprograms.\\nEven high-level languages are affected by the memory hierarchy. Consider the following Python \\nloop:\\nimport time\\nfrom array import array\\nN = 10000\\nstride = 10000\\ndata = array('d', (float(i) for i in range(stride * N)))\\ns = 0.0\\nt0 = time.perf_counter()\\n7\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 7, 'page_label': '8'}, page_content='for i in range(N):                  # visit consecutive elements\\n    s += data[i]\\nt1 = time.perf_counter()\\nprint(\"Sequential time (good spatial locality):\", t1 - t0, \"seconds\")\\nt0 = time.perf_counter()\\nfor i in range(0, stride * N, stride):   # jump by \\'stride\\' each time\\n    s += data[i]\\nt1 = time.perf_counter()\\nprint(\"Strided time (bad spatial locality):\", t1 - t0, \"seconds\")\\nWhen stride = 1, elements are accessed consecutively; each cache line fetched from memory'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 7, 'page_label': '8'}, page_content='contains many useful values. When stride = 64, each access touches a different cache line, forcing \\nmany misses. The program may slow down by an order of magnitude even though the algorithm is \\nidentical. This is a simple, visible effect of spatial locality on performance.\\n1.7 Summary\\n• The memory hierarchy combines small, fast, costly memories near the processor with large, \\nslow, cheap memories farther away.\\n• The hierarchy’s effectiveness depends on the principle of locality.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 7, 'page_label': '8'}, page_content='• Information moves between levels in fixed-size blocks (cache lines).\\n• Performance is measured by the Average Memory Access Time (AMAT).\\n• Even small miss rates can significantly slow execution.\\n• (Non-examinable) Awareness of locality helps programmers write faster, more cache-friendly \\ncode.\\n2 Caches: Organization, Operation, and Performance\\n2.1 From Concept to Hardware\\nA cache is a small, fast memory built from static RAM (SRAM) that stores copies of recently used'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 7, 'page_label': '8'}, page_content='portions of main memory (DRAM). It sits between the CPU and main memory and acts as a buffer \\nto exploit the principle of locality:\\nAMAT = Hit time + Miss rate × Miss penalty.\\nWhen the processor issues a load or store:\\n1. The cache checks if the requested address is already present (a hit).\\n2. If so, data are returned immediately.\\n3. If not (a miss), the entire block of memory containing that address is fetched from the next \\nlower level and stored in the cache.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9'}, page_content='Figure 4: Cache contents immediately before (left) and after (right) an access to a word xn in \\nmemory that is not initially in the cache. The reference causes a miss that forces the cache to fetch \\nxn from memory and insert it into the cache (Fig 5.7 [Patterson & Hennesy]).\\nExtra (non-examinable) SRAM stands for Static RAM, and these are have faster access times \\n(usually 1 cycle). DRAM stands for Dynamic RAM, and these memories have slower access times.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9'}, page_content='SRAMs use 8 transistors per bit of information, whereas DRAMs use capacitors to store a cell \\nvalues, and one transistor per bit to read or write the value stored by a capacitor. Because \\nDRAMs only use one transistor per bit, they are denser and cheaper than SRAMs. However, \\nsince DRAMs contents store data in capacitors, and these eventually lose their charge, they need \\nto be refreshed periodically (i.e. read and written back).\\n2.2 Cache Structure and Address Fields'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9'}, page_content='2.2 Cache Structure and Address Fields\\nA cache holds data in fixed-size blocks (or lines), each containing B bytes. Each block corresponds \\nto a contiguous region of main memory.\\nLet\\nC = cache capacity (bytes), B = block size (bytes), A = associativity (ways).\\nThen the number of sets is\\nS = C\\nB × A.\\nEach cache entry stores:\\n• a data block (B bytes),\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 9, 'page_label': '10'}, page_content='• a tag (the high-order address bits),\\n• a valid bit,\\n• optionally a dirty bit (for write-back caches).\\nAddress breakdown. Given an m-bit address, the fields are:\\nTag⏞⏟⏟⏞\\nidentifies memory block\\n| Index⏞ ⏟⏟ ⏞\\nselects set\\n| Block offset⏞ ⏟⏟ ⏞\\nlocates byte within block\\n.\\nOffset bits = log2(B), Index bits = log2(S), Tag bits = m − (offset + index).\\nWorked Example 2.1 — Address Breakdown\\nA 4 KB cache, 16 B blocks, direct-mapped, 32-bit addresses:\\nS = 4096'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 9, 'page_label': '10'}, page_content='S = 4096\\n16 = 256 = 28 ⇒ offset = 4, index = 8, tag = 20.\\nAddress 0x12AB34CD → lowest 4 bits = offset, next 8 bits = index, top 20 bits = tag.\\nWe can expand the example to see precisely how the address fields are divided.\\nGiven a 32-bit address 0x12AB34CD:\\n0x12AB34CD = 0001 0010 1010 1011 0011 0100 1100 11012\\nWe divide the 32-bit address accordingly (from least to most significant bits):\\n Field  Bit positions  Binary  Hex  Decimal  Meaning\\n Offset  [3..0]  1101  D  13  Byte within 16 B block'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 9, 'page_label': '10'}, page_content='Index  [11..4]  0100 1100  4C  76  Cache line number\\n Tag  [31..12]  0001 0010 1010 1011 0011  12AB3  —  Identifies memory block\\nNotes:\\n• The index crosses a byte boundary: bits [11..8] are the low nibble of 0x34 (= 0x4); bits [7..4] \\nare the high nibble of 0xCD (= 0xC). Together they form 0x4C.\\n• Block base address = 0x12AB34CD& 0xFFFFFFF0 = 0x12AB34C0.\\n• Cache line index = 0x4C = 76.\\n• Tag stored in that line = 0x12AB3.\\nTag = 0x12AB3, Index = 0x4C, Offset = 0xD.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 9, 'page_label': '10'}, page_content='Tag = 0x12AB3, Index = 0x4C, Offset = 0xD.\\nThis explicit division will be used repeatedly when analyzing cache hits and misses in Section 3.\\n2.3 Mapping Strategies: Where Does Each Block Go?\\nThe mapping policy determines which cache location(s) can store a given memory block.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11'}, page_content='Figure 5: A direct-mapped cache with eight entries showing the addresses of memory words between \\n0 and 31 that map to the same cache locations. Because there are eight words in the cache, an \\naddress X maps to the direct-mapped cache word X%8. That is, the low-order log2(8) = 3 bits \\nare used as the cache index. Thus, addresses 00001two, 0b01001, 0b10001, and 0b11001 all map to \\nentry 0b001 of the cache, while addresses 0b00101, 0b01101, 0b10101, and 0b11101 all map to entry'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11'}, page_content='0b101 of the cache. (Fig 5.8 of [Patterson & Hennessy]).\\n1. Direct-Mapped. Each memory block maps to exactly one cache line:\\nline index = (block address) mod N,\\nwhere N = number of lines.\\nHardware:\\n1. Extract index from address → select line.\\n2. Compare stored tag with address tag.\\n3. If equal and valid → hit; else → miss → fetch block → overwrite line.\\nThis is fast (one comparator) but vulnerable to conflict misses.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11'}, page_content='As an example of how a direct-mapped cache works, consider a cache with 8 lines (indexes 0b000 \\nto 0b111) and a block size of one word. Each memory reference is given as a word address, so the \\ncache index is simply (address mod 8). The table below traces a short sequence of addresses and \\nshows for each one:\\n1. the decimal and binary forms of the reference,\\n2. whether the reference is a hit or miss,\\n11'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 11, 'page_label': '12'}, page_content='Figure 6: Example of cache contents after each reference request. (Fig 5.9 in Patterson & Hennesy).\\n3. and the cache line (block) that it maps to.\\nEach line of the table corresponds to one snapshot in the illustration (Fig. 6a – f). Notice that \\naddresses separated by multiples of 8 map to the same cache block, causing conflict misses in a \\ndirect-mapped cache. This simple example captures the essence of cache replacement and locality \\nin direct mapping.\\nDecimal ad\\xad\\ndress of ref\\xad\\nerence'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 11, 'page_label': '12'}, page_content='Decimal ad\\xad\\ndress of ref\\xad\\nerence\\nBinary address \\nof reference\\nHit or miss \\nin cache\\nAssigned cache block \\n(where found or placed)\\n22 0b10110 miss (Fig. 6b) (0b10110 mod 8) = 0b110\\n26 0b11010 miss (Fig. 6c) (0b11010 mod 8) = 0b010\\n22 0b10110 hit (0b10110 mod 8) = 0b110\\n26 0b11010 hit (0b11010 mod 8) = 0b010\\n16 0b10000 miss (Fig. 6d) (0b10000 mod 8) = 0b000\\n3 0b00011 miss (Fig. 6e) (0b00011 mod 8) = 0b011\\n16 0b10000 hit (0b10000 mod 8) = 0b000\\n18 0b10010 miss (Fig. 6f) (0b10010 mod 8) = 0b010'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 11, 'page_label': '12'}, page_content='18 0b10010 miss (Fig. 6f) (0b10010 mod 8) = 0b010\\n16 0b10000 hit (0b10000 mod 8) = 0b000\\n12'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13'}, page_content='2. Set-Associative. Each set contains multiple ways. A memory block maps to exactly one set \\nbut may occupy any of its ways.\\nIf A=number of ways, then\\nS = C\\nB × A.\\nThe index chooses one of the S sets; within that set, A entries exist.\\nInternal structure (the hardware “table” ). Each set is a small table of A entries. Each \\nentry stores:\\n• its tag (high-order bits of the address),\\n• a valid and dirty bit,\\n• the data block.\\nOn every access:\\n1. The index bits select one set.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13'}, page_content='1. The index bits select one set.\\n2. All A tags in that set are read simultaneously and compared (using A comparators).\\n3. If one matches and valid = 1 → hit.\\n4. Otherwise → miss → fetch block from lower level. If all ways are full, choose one line to evict.\\nHence, each set physically contains a tiny parallel table mapping tag → data. Associativity \\nmeans there are multiple such possible entries per index.\\nExample — 2-Way Cache Lookup. 4 KB cache, 16 B blocks, 2-way associative: S ='),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13'}, page_content='4096/(16×2) = 128 sets. Each set holds 2 entries:\\n[Tag0, Valid0, Data0], [Tag1, Valid1, Data1].\\nIf the index = 37, both tags of set 37 are compared with the requested tag. If neither matches, one \\nentry is evicted and replaced.\\nAssociativity reduces conflict misses because several competing blocks with the same index can \\ncoexist within a set.\\n3. Fully Associative.  This is a special case with S = 1. Every block can occupy any line; all'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13'}, page_content='tags (hundreds) are compared in parallel. Hardware cost grows with number of comparators, so \\nthis is used only for small caches such as the TLB.\\n2.4 Replacement Policies\\nWhen a set is full and a miss occurs, one of its A entries must be evicted.\\nLeast Recently Used (LRU): replace the block not accessed for the longest time. Perfect for \\nsmall A (2 – 4). For 2-way caches, a single “recently used” bit per set suffices.\\nRandom: choose any way at random; simple and almost as effective for high A.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13'}, page_content='Pseudo-LRU: approximate LRU using a binary tree of bits (used in 8-way L2 caches).\\nExample — 2-Way LRU. Each set has one “use” bit: when way 0 is used, mark 0 → recent; \\nwhen way 1 is used, mark 1 → recent. On a miss, evict the least-recently used way.\\n2.5 Write Policies\\nHow and when writes propagate to main memory:\\n13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14'}, page_content='Figure 7: Direct mapped, set associative, vs fully associative. The location of a memory \\nblock whose address is 12 in a cache with eight blocks varies for direct-mapped, set-associative, \\nand fully associative placement.In direct-mapped placement, there is only one cache block where \\nmemory block 12 can be found, and that block is given by (12 modulo 8) = 4. In a two-way set-\\nassociative cache, there would be four sets, and memory block 12 must be in set (12 mod 4) = 0; the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14'}, page_content='memory block could be in either element of the set. In a fully associative placement, the memory \\nblock for block address 12 can appear in any of the eight cache blocks. (Fig. 5.14 in [Patterson & \\nHennessy]).\\nWrite-through. Every write updates both cache and next level. +: simpler consistency. –: more \\nmemory traffic.\\nWrite-back. Write only modifies the cache copy; mark entry ’  s dirty bit. When evicted, the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14'}, page_content='whole block is written to lower memory. +: fewer writes to memory. –: requires dirty-bit tracking \\nand write-back on replacement.\\nWrite-allocate vs. No-write-allocate. On a write miss:\\n• Write-allocate: fetch block into cache, then write → used with write-back.\\n• No-write-allocate: write directly to memory, skip cache → used with write-through.\\n2.6 Types of Misses (3C Model)\\nAll cache misses fall into one of three classes (Fig. 5.10):\\nCompulsory: first reference to a block.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14'}, page_content='Compulsory: first reference to a block.\\nCapacity: working set larger than cache capacity.\\nConflict: multiple blocks mapping to same set.\\nIncreasing block size reduces compulsory misses (spatial locality); increasing total capacity re\\xad\\nduces capacity misses; increasing associativity reduces conflict misses.\\n2.7 Quantitative Analysis: Average Memory Access Time\\nAMAT = thit + rmiss × tpenalty.\\n14'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15'}, page_content='Figure 8: An eight-block cache configured as direct mapped, two-way set associative, four-way set \\nassociative, and fully associative. (Fig. 5.15 in Patterson & Hennessy).\\nExample 1 – Single Level. Hit = 1 cycle, miss rate = 5 %, penalty = 50 cycles ⇒ AMAT = \\n1 + 0.05×50 = 3.5 cycles.\\nExample 2 – Two Levels. L1: 1 cycle, 5 % miss; L2: 10 cycles, 10 % of L1 misses; Memory: \\n100 cycles:\\nAMAT = 1 + 0.05(10 + 0.1 × 100) = 2.0 cycles.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15'}, page_content='AMAT = 1 + 0.05(10 + 0.1 × 100) = 2.0 cycles.\\nExample 3 – Associativity Trade-off. Direct-mapped: hit = 1 cycle, miss rate = 5 %; \\n2-way: hit = 2 cycles, miss rate = 3 %; penalty = 50 cycles.\\nAMATDM = 1 + 0.05 × 50 = 3.5, AMAT2-way = 2 + 0.03 × 50 = 3.5.\\nHere associativity removes conflicts but increases hit time; overall effect can balance.\\n2.8 Pipeline Interaction: Instruction vs. Data Cache\\nIn pipelined CPUs, both instruction fetch (IF) and data memory (MEM) stages access memory'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15'}, page_content='each cycle. To avoid conflicts, most designs split L1 into:\\n• an Instruction cache (I-cache) – read-only for fetching instructions,\\n• a Data cache (D-cache) – read/write for load-store data.\\nLower levels (L2/L3) are unified. Thus, IF and MEM can proceed simultaneously without con\\xad\\ntention.\\n15'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 15, 'page_label': '16'}, page_content='Figure 9: Illustrative only! Building a 4-way set associative cache.\\nAdditional Contents (non-examinable): Locality and Data Layout in High-Level \\nLanguages\\nCache principles apply to all programming languages through memory layout.\\nRow-major vs. Column-major order.\\n• Row-major (C, C++, Python/NumPy): consecutive elements of each row are contigu\\xad\\nous.\\n• Column-major (Fortran, MATLAB): consecutive elements of each column are contiguous.\\nTraversing memory in storage order improves spatial locality.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 15, 'page_label': '16'}, page_content='Arrays of arrays. Python lists of lists store each row as a separate object, possibly far apart in \\nmemory. NumPy arrays store one contiguous buffer, allowing efficient cache use.\\nExample – Matrix Multiplication Order.\\nfor i in range(N):\\n    for j in range(N):\\n        for k in range(N):\\n            C[i][j] += A[i][k] * B[k][j]\\n16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 16, 'page_label': '17'}, page_content='This version scans B by column → poor locality in row-major storage. Reordering:\\nfor i in range(N):\\n    for k in range(N):\\n        aik = A[i][k]\\n        for j in range(N):\\n            C[i][j] += aik * B[k][j]\\nnow scans B row-wise → better cache reuse.\\nKey ideas.\\n• Matching loop order to array layout improves spatial locality.\\n• Small contiguous “working sets” exploit temporal locality.\\n• Understanding these patterns helps design faster software even in high-level languages.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 16, 'page_label': '17'}, page_content='Additional (non-examinable): Typical Parameters in Real Systems\\n Level  Typical size  Associativity  Hit ∼ cycles  Notes\\n L1 I/D  32 – 64 KiB  4 – 8  3 – 5  Split I/D, often VIPT\\n L2  256 KiB – 2 MiB  4 – 8  10 – 20  Unified\\n L3  8 – 64 MiB  8 – 16  30 – 50+  Shared in multicore\\n TLB L1  32 – 128 entries  4 – 8 ≈ 1  Separate I/D common\\n DRAM — —  50 – 200  Depends on freq/DDR gen\\nIllustrative only; not examinable.\\n2.9 Summary of Section 2'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 16, 'page_label': '17'}, page_content='2.9 Summary of Section 2\\n• Cache = small, fast SRAM memory that stores blocks of main memory.\\n• Each address divides into tag / index / offset.\\n• Mapping strategies:\\n– Direct-mapped – one line per set (simple, high conflict risk).\\n– Set-associative – multiple ways per set (balanced).\\n– Fully associative – any line (flexible, expensive).\\n• Each set physically stores a small table of entries (tag, valid, data). On access, all tags in that \\nset are compared in parallel.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 16, 'page_label': '17'}, page_content='set are compared in parallel.\\n• Replacement policies: LRU, Random, Pseudo-LRU.\\n• Write policies: write-through / write-back, write-allocate / no-write-allocate.\\n• AMAT quantitatively measures performance; associativity and block size affect both hit time \\nand miss rate.\\n• Split I/D caches prevent pipeline conflicts.\\n• (Non-examinable) Locality awareness in software leads to cache-efficient algorithms.\\n17'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18'}, page_content='Concept check (Section 2)\\n1. If we double block size, which 3C miss typically decreases first? Compulsory (spatial locality).\\n2. What parameter primarily reduces conflict misses? Higher associativity.\\n3. In a direct-mapped cache, what determines the line index? Block number mod #lines.\\nCommon pitfalls\\n• Do not mix hex nibbles with bit fields; rewrite addresses in binary when counting bits.\\n• Keep units straight: cycles vs ns; restate the frequency when converting.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18'}, page_content='• For set-associative caches, tags are per way in the selected set; compare all ways in parallel.\\nPrefetching (non-examinable). Hardware prefetchers fetch the next line (or detect short strides) \\nto reduce compulsory misses; software may issue prefetch hints. Prefetching helps when access pat\\xad\\nterns are predictable.\\nMulticore note (non-examinable). Real systems enforce coherence so cores see a consistent'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18'}, page_content='view of memory. Our single-core cache model remains valid for understanding locality, mapping, \\nand AMAT; coherence adds additional traffic and latency not covered here.\\n3 Exercises: Caches\\n3.1 Purpose and Overview\\nThis section consolidates the principles of cache design through quantitative practice. The exercises \\nemphasize direct-mapped caches—because they are the simplest to analyze—and Average Memory'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18'}, page_content='Access Time (AMAT) evaluation under varying parameters. Worked examples are written step by \\nstep; the remaining exercises are intended for individual practice. A final subsection offers a few \\noptional, more advanced problems on associative caches and replacement policies.\\n3.2 Worked Example 3.1 — Direct-Mapped Cache Indexing and Miss Pattern\\nProblem. A processor uses a 32-bit address space and a 2 KB direct-mapped data cache with'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18'}, page_content='32-byte blocks. The following byte addresses are accessed in order:\\n0x0000, 0x0020, 0x0040, 0x0060, 0x0000, 0x0020.\\nDetermine for each access whether it is a hit or a miss, and explain why.\\nStep 1 – Compute parameters.\\n2048 B cache\\n32 B per block = 64 blocks (cache lines) = 26.\\nTherefore:\\nOffset bits = log2(32) = 5, Index bits = log2(64) = 6, Tag bits = 32 − 6 − 5 = 21.\\nEach address can be divided as:\\ntag[31:11]⏞ ⏟⏟ ⏞\\n21 bits\\n| index[10:5]⏞ ⏟⏟ ⏞\\n6 bits\\n| block offset[4:0]⏞ ⏟⏟ ⏞\\n5 bits\\n.\\n18'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 18, 'page_label': '19'}, page_content='Step 2 – Determine block numbers. A block is 32 bytes, so each consecutive block starts 32 \\nbytes (0x20 in hexadecimal) after the previous one. We can find the block number by dividing the \\naddress by 32.\\n0x0000/32 = 0 ⇒ block number 0,\\n0x0020/32 = 1 ⇒ block number 1,\\n0x0040/32 = 2 ⇒ block number 2,\\n0x0060/32 = 3 ⇒ block number 3.\\nStep 3 – Compute cache line indices. Because the cache is direct-mapped, each block can only \\ngo in one line, found by:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 18, 'page_label': '19'}, page_content='go in one line, found by:\\nCache line index = (block number) mod (number of lines).\\nThere are 64 lines, so mod64 selects the lower 6 bits of the block number.\\nBlock 0 ⇒ 0 mod 64 = line 0,\\nBlock 1 ⇒ 1 mod 64 = line 1,\\nBlock 2 ⇒ 2 mod 64 = line 2,\\nBlock 3 ⇒ 3 mod 64 = line 3.\\nTherefore, 0x0020 maps to cache line 1 because it belongs to block 1, and\\n1 mod 64 = 1.\\nEach successive block occupies the next cache line in order.\\nStep 4 – Trace the sequence.\\n• 0x0000: miss → fill line 0 (tag 0)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 18, 'page_label': '19'}, page_content='• 0x0000: miss → fill line 0 (tag 0)\\n• 0x0020: miss → fill line 1 (tag 0)\\n• 0x0040: miss → fill line 2 (tag 0)\\n• 0x0060: miss → fill line 3 (tag 0)\\n• 0x0000: hit (tag 0 still in line 0)\\n• 0x0020: hit (tag 0 still in line 1)\\nStep 5 – Summary. The first access to each block causes a compulsory miss (first reference to \\nthat block). Subsequent accesses to the same block hit because the corresponding cache line still \\ncontains that data.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 18, 'page_label': '19'}, page_content='contains that data.\\n3.3 Worked Example 3.2 — Conflict Misses in a Direct-Mapped Cache\\nProblem. Same configuration (2 KB direct-mapped, 32 B blocks). Access sequence:\\n0x0000, 0x0800, 0x1000, 0x0000, 0x1000\\nStep 1 – Mapping conflict. Cache has 64 lines → index = 6 bits. Blocks differing by multiples \\nof 64 × 32 = 2048 B map to the same line.\\nStep 2 – Pattern. Addresses differ by 0x0800 = 2048 B → same index, different tags.\\n19'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 19, 'page_label': '20'}, page_content='Step 3 – Trace.\\n• 0x0000: miss (tag 0)\\n• 0x0800: miss (tag 1, replaces 0)\\n• 0x1000: miss (tag 2, replaces 1)\\n• 0x0000: miss again (tag 0 ≠ 2)\\n• 0x1000 (final access): hit (still resident from previous step).\\nConclusion. Each access evicts the previous one —pure conflict misses. Such pathologies mo\\xad\\ntivate set-associative caches or address interleaving in software.\\n3.4 Worked Example 3.3 — AMAT Under Different Parameters\\nProblem. Compare the effect of different cache parameters on performance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 19, 'page_label': '20'}, page_content='Case A: L1 hit = 1 cycle, miss rate = 5 %, penalty = 50 cycles. Case B: same cache but miss \\nrate = 2 % (larger cache). Case C: same miss rate as A but penalty = 30 cycles (faster memory).\\nStep 1 – Formula.\\nAMAT = thit + rmiss × tpenalty.\\nStep 2 – Compute.\\nA: 1 + 0.05 × 50 = 3.5,\\nB: 1 + 0.02 × 50 = 2.0,\\nC: 1 + 0.05 × 30 = 2.5.\\nInterpretation. Reducing miss rate or penalty both improve AMAT, but their relative benefit'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 19, 'page_label': '20'}, page_content='depends on which term dominates. Hardware (memory speed) and architecture (cache size) thus \\ninteract directly in total CPI.\\n3.5 Worked Example 3.4 — Address Fields in a Direct-Mapped Cache\\nProblem. A 1 KiB direct-mapped cache with 16 B blocks receives address 0xABCD. Find the tag, \\nindex, and offset fields assuming 16-bit addresses.\\nParameters. Cache size = 210 B, block size = 24 B. Number of lines = 210/24 = 26. Hence: \\noffset = 4 bits, index = 6 bits, tag = 16 − 6 − 4 = 6 bits.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 19, 'page_label': '20'}, page_content='Binary breakdown. 0xABCD = 1010 1011 1100 11012.\\nTag[15 : 10] = 101010 (0x2A), Index[9 : 4] = 111100 (0x3C), Offset[3 : 0] = 1101 (0xD).\\nInterpretation. Index = 0x3C = 60 selects cache line 60, tag = 0x2A must match for a hit, and \\nthe byte is at offset 13 within its 16 B block.\\n20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='3.6 Exercises\\nThe following problems build intuition for direct-mapped cache behavior and AMAT estimation. \\nWork them carefully by hand before using any simulator.\\n3.6. Basic Tag/Index/Offset Computation. 32-bit address, 4 KB direct-mapped cache, 16 B \\nblocks. Find tag, index, and offset sizes. Hint: 4096/16 = 256 lines → 8 index bits.\\n3.6. Address Tracing 1. Same cache; access sequence 0x00, 0x10, 0x20, 0x00, 0x30. Determine'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='hits/misses and classify the miss types. Hint: Each block = 16 B; draw a small table of line \\n↔ tag.\\n3.6. Address Tracing 2. 2 KB cache, 32 B blocks, direct-mapped. Accesses 0x0000, 0x0800, \\n0x1000, 0x1800. Predict the pattern. Hint: Stride = cache size → conflict every time.\\n3.6. Block Size Effect on AMAT. Hit = 1 cycle, miss rate = 8 % with 16 B blocks; 32 B \\nblocks reduce miss rate to 5 % but raise hit time to 2 cycles. Compute both AMATs. Hint:\\n1 + 0.08M vs. 2 + 0.05M with same penalty M.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='1 + 0.08M vs. 2 + 0.05M with same penalty M.\\n3.6. Impact of Memory Speed. Hit = 1 cycle, miss rate = 4 %, penalty = 100 cycles. If faster \\nDRAM halves penalty, what is the percentage speed-up? Hint: Compare old/new AMAT.\\n3.6. Nested AMAT. L1: hit 1 cycle, miss 5 %; L2: hit 10 cycles, miss 10 % of L1 misses; Memory \\n= 100 cycles. Compute overall AMAT. Hint: 1 + 0.05(10 + 0.1 × 100).\\n3.6. Performance Scaling. For a CPU at 3 GHz, compute nanoseconds per access for AMAT ='),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='2 cycles. Hint: 1 cycle = 1/3 ns.\\nOptional (Advanced): Associativity and Replacement Policies\\nThe following problems extend the analysis to associative caches and realistic policies. They are \\nrecommended for deeper understanding but are not examinable.\\nO3.6. 2-Way Set-Associative Mapping. 8 KB cache, 16 B blocks, 2-way associative. Compute \\ntag/index/offset bits and explain how associativity changes miss patterns compared to direct-\\nmapped.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='mapped.\\nO3.6. Replacement Policy Trace. 2-way cache, LRU replacement. Access sequence: A, B, A, \\nC, A, B. Which blocks are evicted at each step? Hint: Track recency of each set.\\nO3.6. Write-Back vs. Write-Through Timing. Suppose 25 % of accesses are writes; memory \\nwrite takes 40 cycles. Compare total memory traffic for write-through and write-back policies \\nassuming 5 % miss rate. Hint: Only dirty blocks cause writes in write-back.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='O3.6. Effect of Associativity on AMAT. L1 direct-mapped: miss rate = 5 %; L1 2-way: miss \\nrate = 3 %, hit time +1 cycle. Compute which is faster given 50-cycle penalty. Hint: Compare \\nAMATs quantitatively.\\n3.7 Concluding Remarks\\nThese problems illustrate how cache performance arises from quantitative interactions among block \\nsize, cache size, associativity, and memory speed. Always begin with a clear understanding of the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21'}, page_content='address breakdown and the hierarchy’  s timing model. Mastery of direct-mapped caches forms the \\nfoundation for the virtual-memory mechanisms studied next.\\n21'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='4 Virtual Memory and Address Translation\\nBefore we start: the tiny translator in the CPU\\nWhen a program uses memory, it asks for a location by giving a number. This number is the \\nprogram’s own way of naming a location; it is not yet a real slot in the computer’s memory chips.\\nInside the processor there is a small hardware translator that turns the program’s number into \\na real location in memory. This translator is called the Memory Management Unit (MMU).\\nAt a high level:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='At a high level:\\n• The program provides a number we will call a virtual address.\\n• The MMU converts it into a physical address, which is where the data actually live in DRAM.\\n• To know how to convert, the MMU consults a table prepared by the operating system (we \\nwill define this table in the next pages).\\n• To go fast, the MMU also keeps a few recent conversions in a tiny hardware cache; we will \\ngive this cache a name later in this section.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='give this cache a name later in this section.\\nAfter this brief overview, we will now define these terms carefully (virtual address, physical \\naddress, pages, the table, and the small cache) and work through step-by-step examples.\\n4.1 Motivation\\nUp to this point, we have worked with physical addresses—the real locations of data in memory. \\nModern computer systems introduce an additional layer called virtual memory, which allows each'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='running program to behave as though it has a large, private, and contiguous memory space.\\nThis layer serves three essential purposes:\\n1. Isolation: each process runs in its own protected address space.\\n2. Protection: one program cannot accidentally overwrite another’s data.\\n3. Flexibility: programs can use large contiguous virtual spaces even when physical memory is \\nfragmented or partly stored on disk.\\nVirtual memory is implemented through cooperation between the processor hardware and a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='software component of the system called the operating system (OS). The hardware part performs \\nfast address translation; the OS manages which parts of each program are currently loaded in \\nmemory.\\nCPU (virtual address) → MMU (hardware translation) → Physical Memory\\n4.2 Pages and Frames\\nBoth virtual and physical memories are divided into fixed-size blocks:\\n• A page is a contiguous block of virtual memory.\\n• A frame (or physical page) is an equally sized block in physical memory.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22'}, page_content='Common page sizes are 4 KB, 8 KB, or 2 MB. Each virtual address is divided into two parts:\\nVirtual Page Number (VPN)⏞ ⏟⏟ ⏞\\nhigh bits\\n| Page offset⏞ ⏟⏟ ⏞\\nlow bits\\n.\\n22'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 22, 'page_label': '23'}, page_content='Figure 10: The concept of “Virtual Memory”. In virtual memory, blocks of memory (called pages) \\nare mapped to physical addresses. Both virtual and physical memory are split into pages (1 virtual \\npage maps to 1 physical page). Processors generate virtual addresses, while memory is accessed \\nusing physical addresses. A hardware component called the MMU, or Memory Management Unit, \\nis in charge of doing this translation (Fig 5.24 Patterson & Hennessy).'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 22, 'page_label': '23'}, page_content='If the virtual address has m bits and each page has 2p bytes, then:\\nVPN bits = m − p, Offset bits = p.\\nThe offset selects a byte within a page (it is copied unchanged into the physical address). The \\nVPN is used to index a data structure called the page table, which holds the corresponding physical \\nframe number (PFN).\\n23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 23, 'page_label': '24'}, page_content='Figure 11: Translating a virtual address to a physical address. (Fig 5.25 Patterson & Hennessy)\\nExtra contents:\\nOnce we define a page as a fixed-size unit of virtual memory (for example 4 KiB), several design \\nchoices arise:\\n• Page size. Pages should be large enough that the long access time to secondary storage \\n(for example, disk) is amortized over a reasonably large transfer. Typical page sizes today \\nrange from 4 KiB to 16 KiB. New desktop and server systems are moving toward 32 KiB'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 23, 'page_label': '24'}, page_content='or even 64 KiB pages, while embedded systems often use smaller pages such as 1 KiB to \\nsave space.\\n• Reducing page-fault rate. It is desirable to minimize page faults because they involve \\nextremely high latency. One effective organization is to allow fully associative placement of \\npages in physical memory—any virtual page can be placed in any physical frame—so that \\nno page is forced out due to simple placement conflicts.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 23, 'page_label': '24'}, page_content='• Handling page faults. Because servicing a page fault already involves a very slow disk \\naccess, it is efficient to let software handle page faults and replacement decisions. The \\noverhead of software management is negligible compared to the disk latency, and it allows \\nthe use of sophisticated algorithms that can further reduce the miss rate.\\n• Write policy. A write-through policy is impractical for virtual memory, since writing to'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 23, 'page_label': '24'}, page_content='disk on every store would be far too slow. Therefore, virtual memory systems always use \\na write-back policy: modified pages are marked as “dirty” and written to disk only when \\nthey are replaced.\\n24'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 24, 'page_label': '25'}, page_content='Figure 12: The Page Table. (Fig 5.26 Patterson & Hennessy).\\nWorked Example 4.1 – Basic Translation\\nConsider a 16-bit virtual address and 1 KB pages (210 bytes). Then:\\nVPN bits = 6, Offset bits = 10.\\nSuppose that virtual page 5 maps to physical frame 12. For address 0x15A3:\\n0x15A3 = 000101⏞ ⏟⏟ ⏞\\nVPN=5\\n10100011⏞ ⏟⏟ ⏞\\noffset=0xA3\\n.\\nReplacing the VPN with the PFN:\\nPA = PFN ||offset = 00001100 ||10100011 = 0xCA3.\\nThus, virtual address 0x15A3 corresponds to physical address 0xCA3.\\n4.3 The Page Table'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 24, 'page_label': '25'}, page_content='4.3 The Page Table\\nThe page table is a data structure maintained by the operating system but used by hardware during \\nmemory access. Each process has its own page table.\\nEach Page Table Entry (PTE) contains:\\n25'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26'}, page_content='• The physical frame number (PFN),\\n• A valid bit (1 if the page is currently loaded in memory),\\n• Protection bits (read/write/execute),\\n• Optional flags such as dirty and reference bits for the OS.\\nWhen a program accesses memory:\\n1. The Memory Management Unit(MMU) extracts the virtual page number.\\n2. It looks up the corresponding page table entry.\\n3. If valid = 1, the page is in memory and translation succeeds.\\n4. If valid = 0, a page fault occurs, and the OS loads the page from disk.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26'}, page_content='Connection to caches. The page table plays a similar role to a cache directory: both map logical \\naddresses (VPNs or block numbers) to physical locations.\\nWorked Example 4.2 – Page Table Lookup\\nAssume a 32-bit virtual address, 4 KB pages (212 bytes). VPN = upper 20 bits, offset = lower 12 \\nbits. If VPN = 0x00020 maps to PFN = 0x00007, then:\\nVA = 0x00020ABC → PA = 0x00007ABC.\\nOnly the upper bits change; the offset is preserved.\\nExtra:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26'}, page_content='Extra:\\nA single-level page table can be very large. For example, a 64-bit address space with 4 KB pages \\nwould require 252 entries. To manage this, systems use multi-level page tables.\\nEach level acts like an index into a smaller table describing a portion of the address space. This \\nstructure drastically reduces the total size, because only the levels corresponding to used portions \\nof memory need to exist.\\n4.4 The Translation Lookaside Buffer (TLB)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26'}, page_content='4.4 The Translation Lookaside Buffer (TLB)\\nAccessing the page table for every memory reference would be far too slow. To avoid this, the MMU \\nincludes a small, very fast cache of recent translations called the Translation Lookaside Buffer \\n(TLB).\\nEach TLB entry stores:\\n• The virtual page number (as the tag),\\n• The corresponding physical frame number,\\n• Access permissions and valid bit.\\nOperation.\\n1. On every memory access, the MMU checks the TLB for the VPN.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26'}, page_content='2. On a TLB hit, the PFN is returned immediately.\\n3. On a TLB miss, the hardware or OS must consult the page table, insert the translation into \\nthe TLB, and retry the access.\\n26'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 26, 'page_label': '27'}, page_content='Figure 13: Virtual addresses are larger than physical addresses. That means we cannot store all \\nvirtual pages in physical pages. (Fig 5.27 Patterson & Hennessy).\\nWorked Example 4.3 – Effective Memory Access Time (EMAT)\\nAssume:\\n• TLB hit time = 1 ns,\\n• TLB hit rate = 99%,\\n• Memory access time = 100 ns.\\nEMAT = (0.99)(1 + 100) + (0.01)(1 + 200) = 102 ns.\\nEven with a small miss rate, the extra translation time is noticeable.\\n4.5 Page Faults and the Role of the Operating System'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 26, 'page_label': '27'}, page_content='When the page table indicates that a page is not in memory (valid = 0), the MMU raises a page \\nfault. At this point, the operating system must intervene.\\nAt a high level what happens is the following:\\n1. The processor temporarily stops the running program.\\n2. The Operating System locates the missing page on disk.\\n3. It copies the page into a free physical frame in RAM.\\n27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 27, 'page_label': '28'}, page_content='Figure 14: The TLB. (Fig 5.28 Patterson & Hennessy).\\n4. The OS updates the page table entry to make it valid.\\n5. The processor restarts the instruction that caused the fault.\\nFrom the program’s point of view, this all happens automatically, but a page fault is millions \\nof times slower than a normal memory access. This is why programs that reuse recently accessed \\ndata (good locality) perform much better.\\n4.6 Interaction with the Cache Hierarchy\\nCaches and virtual memory operate together:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 27, 'page_label': '28'}, page_content='Caches and virtual memory operate together:\\n• The MMU translates virtual to physical addresses before cache access.\\n• Caches store data using physical addresses to avoid confusion between processes.\\n• The TLB itself behaves as a tiny associative cache for page table entries.\\nIn pipelined processors, address translation and cache lookup are performed in parallel to avoid \\nslowing down memory access.\\n4.7 Summary of Section 4'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 27, 'page_label': '28'}, page_content='4.7 Summary of Section 4\\n• Virtual memory maps each process’s virtual addresses to physical memory frames.\\n• The MMU performs this translation automatically on every memory access.\\n• The page table holds the mappings; the TLB caches the most recent ones.\\n28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 28, 'page_label': '29'}, page_content='Figure 15: Virtual Memory and Cache working together to fetch a memory block. (Fig 5.29 Pat\\xad\\nterson & Hennessy).\\n• Page faults occur when data must be fetched from disk; the OS manages this.\\n• Locality is just as important here as in caching: good locality means fewer TLB misses and \\npage faults.\\nConcept check (Section 4)\\n1. Which bits are preserved from VA to PA? Page offset.\\n2. What does a TLB store? Mappings from VPN to PFN with permissions.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 28, 'page_label': '29'}, page_content='3. What triggers a page fault? PTE valid bit = 0 for the referenced VPN.\\n29'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30'}, page_content='5 Assembly Programming and the Integrated Memory View\\n5.1 Why Addressing Modes Matter\\nAt the hardware level, every instruction must specify where its operands come from and where the \\nresult goes. The patterns by which instructions access data are called addressing modes.\\nThese modes determine:\\n• whether the operand comes from a register or from memory,\\n• how the memory address is computed,\\n• how many memory accesses are required, and therefore'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30'}, page_content='• how much time the instruction will take when executed on a pipelined processor with caches.\\nEfficient use of addressing modes is central to writing code that respects the memory hierarchy. \\nUnderstanding them also helps interpret compiler output and reason about memory locality.\\n5.2 MIPS Addressing Modes\\nThe MIPS architecture has a simple and regular design. Each instruction fits in 32 bits, and memory'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30'}, page_content='is accessed only by explicit load (‘lw‘, ‘lh‘, ‘lb‘) and store (‘sw‘, ‘sh‘, ‘sb‘) operations.\\n1. Immediate addressing. The operand is a constant encoded directly in the instruction.\\naddi $t0, $t1, 4     # $t0 = $t1 + 4\\nNo memory access occurs here: the constant 4 is part of the instruction itself.\\n2. Register addressing. Both operands come from registers.\\nadd $s0, $s1, $s2   # $s0 = $s1 + $s2\\nAgain, this uses only the register file; no data cache access.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30'}, page_content='3. Base (displacement) addressing. A constant offset is added to a register to form a memory \\naddress.\\nlw $t0, 8($s1)       # load word from address ($s1 + 8) into $t0\\nsw $t0, 0($s2)       # store word from $t0 to address ($s2 + 0)\\nThis is the standard way to access arrays or structure fields in MIPS. If $s1 holds the base address \\nof an array, the offset selects an element.\\n4. PC-relative addressing.  Used for branches and jumps: the new program counter (PC) is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30'}, page_content='computed as the current PC plus a signed offset.\\nbeq $t0, $t1, target   # if equal, PC = PC + offset to target\\nThis mode improves code locality: short branches typically target nearby addresses, which remain \\nin the instruction cache.\\n5. Pseudo-direct addressing.  Used by the ‘j‘ and ‘jal‘ instructions. The target address is \\nformed by combining high bits of the current PC with a 26-bit field in the instruction.\\njal 0x00400000          # jump and link to fixed address\\n30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31'}, page_content='5.3 x86_64 Addressing Modes (for comparison)\\nWhile MIPS keeps addressing simple, x86_64 provides richer options. You are not required to \\nmemorize them for the exam, but learning their structure helps when reading compiler output.\\nIn AT&T syntax (used by GNU assemblers), the general form of a memory operand is:\\ndisp(base, index, scale)\\nwhich corresponds to the effective address:\\nEA = disp + base + index × scale.\\nExamples:\\nmovq  $8(%rax), %rbx        # base + displacement'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31'}, page_content='movq  $8(%rax), %rbx        # base + displacement\\nmovq  (%rax,%rcx,4), %rdx   # base + index * scale\\nmovq  $16(%rbp,%rcx,8), %rax\\n• ‘disp‘ is an immediate displacement (can be zero).\\n• ‘base‘ is a general-purpose register.\\n• ‘index‘ is an optional register multiplied by a scale factor (1, 2, 4, or 8).\\nThese allow direct addressing of array elements or structure fields in memory. For instance, if ‘\\nConnection with MIPS. The x86_64 effective address formula combines the base and index'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31'}, page_content='computation that MIPS expresses using arithmetic instructions before a load. In both architectures, \\nthe final computed address passes through the same cache hierarchy and translation stages discussed \\nearlier.\\n5.4 Efficiency and the Memory Hierarchy\\nFrom the memory-system perspective, instructions differ in cost:\\n• Register and immediate addressing: no memory access; fastest.\\n• Base/displacement addressing: one data cache access; slower.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31'}, page_content='• Indirect addressing (in x86): may cause multiple dependent memory accesses.\\nPrograms that repeatedly use data already in registers or in nearby memory exhibit temporal \\nand spatial locality. The cache system can then deliver data quickly, and few TLB or page-table \\nlookups are needed.\\nExample: Loop locality.\\nLoop:\\n    lw  $t0, 0($s0)\\n    addi $s0, $s0, 4\\n    add  $t1, $t1, $t0\\n    bne  $s0, $s2, Loop'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31'}, page_content='add  $t1, $t1, $t0\\n    bne  $s0, $s2, Loop\\nIf $s0 and $s2 delimit consecutive memory locations, accesses are sequential, benefiting from spatial \\nlocality. If the loop instead followed pointers scattered across memory, cache and TLB misses would \\nincrease dramatically.\\n31'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32'}, page_content='6 Exercises: Virtual Memory\\nWorked Example 6.1 – Virtual to Physical Address Translation\\nA system uses 16-bit virtual addresses and 1 KB pages ( 210 bytes). Physical memory contains 8 \\nframes.\\n1. Split the virtual address into its fields:\\nVPN (6 bits) | Offset (10 bits).\\n2. Suppose the page table contains:\\n VPN  Valid  PFN\\n 0  1  3\\n 1  0  –\\n 2  1  6\\n 3  1  4\\n 4  0  –\\n 5  1  2\\n3. Translate virtual address 0x15A3 step by step:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32'}, page_content='3. Translate virtual address 0x15A3 step by step:\\n(a) Page size = 1 KB = 210. Offset = low 10 bits = 0x0A3. VPN = bits [15:10] = 5.\\n(b) In the page table: VPN 5 → PFN 2 (valid = 1).\\n(c) Replace the VPN with the PFN, keep the same offset:\\nPA = (PFN ≪ 10) + offset = (2 × 1024) + 0x0A3 = 0x08A3.\\nResult: Virtual address 0x15A3 maps to physical 0x08A3.\\nWorked Example 6.2 – TLB Lookup\\nAssume the same address space (16 bits, 1 KB pages). A 4-entry TLB stores recent translations:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32'}, page_content='Entry  VPN  PFN  Valid\\n 0  0  3  1\\n 1  2  6  1\\n 2  5  2  1\\n 3  7  4  1\\nTranslate virtual address 0x15A3:\\n1. Extract VPN = 5.\\n2. Search the TLB: entry 2 matches (hit).\\n3. PFN = 2 → Physical address = 0x08A3.\\nIf VPN 5 were not found, the MMU would check the page table in memory (a miss).\\n32'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 32, 'page_label': '33'}, page_content='Worked Example 6.3 – Effective Memory Access Time (EMAT)\\nParameters:\\n• Memory access = 100 ns\\n• TLB hit time = 1 ns\\n• TLB hit rate = 95 %\\n• Page-table lookup = one extra memory access on miss\\nEMAT = (0.95)(1 + 100) + (0.05)(1 + 200) = 106 ns.\\nEven a small miss rate noticeably slows access.\\n6.1 Exercises\\nEach question below asks for a concrete operation: perform the translation, indicate hit/miss, or \\ncompute a value.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 32, 'page_label': '33'}, page_content='compute a value.\\n6.1. Address Breakdown. 32-bit address, 4 KB pages. (a) How many bits are used for the page \\noffset? (b) How many bits remain for the VPN?\\n6.1. Page Table Translation. 16-bit virtual addresses, 1 KB pages. Given the page table:\\n VPN  Valid  PFN\\n 0  1  5\\n 1  1  3\\n 2  0  –\\n 3  1  1\\nTranslate the following virtual addresses or state that a page fault occurs:\\n• (a) 0x03A2\\n• (b) 0x0D10\\n• (c) 0x07F5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 32, 'page_label': '33'}, page_content='• (a) 0x03A2\\n• (b) 0x0D10\\n• (c) 0x07F5\\n6.1. TLB Lookup. The TLB below contains recent entries. Determine for each virtual address \\nwhether the lookup hits or misses, and if it hits, give the physical address.\\n Entry  VPN  PFN  Valid\\n 0  0x04  0x08  1\\n 1  0x07  0x02  1\\n 2  0x0A  0x05  1\\n 3  0x0C  0x06  1\\nVirtual addresses (hex): (a) 0x0A14�(b) 0x0710�(c) 0x0408�(d) 0x0D30\\nAssume page size = 1 KB.\\n6.1. Compute EMAT. Memory access = 120 ns, TLB access = 2 ns, TLB hit rate = 97%. On a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 32, 'page_label': '33'}, page_content='TLB miss, the page-table lookup costs one extra memory access. (a) Write the exact EMAT \\nformula you are using. (b) Compute EMAT. (c) If hit rate improves to 98.5%, what is the \\npercent speed-up?\\n33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34'}, page_content='6.1. Page Fault Recognition. Given a page table with a valid bit array [1, 1, 0, 1, 0, 1], indicate \\nfor VPN = 2 whether access succeeds or triggers a page fault. What component (hardware \\nor OS) handles it?\\n6.1. Physical Memory Size. Physical address = 18 bits, page size = 4 KB. (a) How many \\nframes exist? (b) What is the total physical memory capacity?\\n(Optional) Hierarchical Lookup. A 32-bit virtual address is split into: 10 bits top-level index,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34'}, page_content='10 bits second-level index, 12 bits offset. What portion of the virtual address space does each \\ntop-level entry describe?\\n7 Integration and Summary\\n7.1 The Complete Memory Hierarchy\\nAll modern systems organize memory in levels to balance speed and cost:\\nCPU registers → L1 cache → L2/L3 cache → Main memory (DRAM) → Disk or SSD (storage).\\nEach level:\\n• stores copies of data from lower levels,\\n• is smaller and faster than the next,\\n• exploits temporal and spatial locality.\\n7.2 Address Flow'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34'}, page_content='7.2 Address Flow\\nWhen the CPU executes a load or store:\\n1. The instruction generates a virtual address.\\n2. The MMU translates it to a physical address (typically using a TLB).\\n3. The cache checks whether the block containing that physical address is present (hit or miss).\\n4. On a hit, data are returned; on a miss, the block is fetched from main memory.\\n5. If the page is not resident (valid bit = 0), a page fault occurs; the OS loads the page and'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34'}, page_content='updates the page table before the access can proceed.\\nWorked Example 7.1 – End-to-End Reference\\nA MIPS processor issues a load from virtual address 0x00402A10. Assume 4 KB pages.\\n1. Split VA into VPN and offset: VPN = 0x00402, offset = 0x0A10.\\n2. TLB lookup for VPN 0x00402 hits with PFN 0x0017.\\n3. Physical address = 0x0017A10.\\n4. The data cache probes the set indexed by this physical address; if the tag matches and valid \\n= 1, it is a cache hit and the load completes.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34'}, page_content='= 1, it is a cache hit and the load completes.\\n5. If the cache misses, the block containing 0x0017A10 is fetched from DRAM into the cache; \\nthe load then completes.\\n34'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35'}, page_content='7.3 Design Trade-offs\\n• Cache parameters: block size, total size, associativity, and replacement policy influence hit \\nrate and hit time (and therefore AMAT).\\n• Write policies: write-through vs. write-back and write-allocate vs. no-write-allocate affect \\nbandwidth and latency.\\n• Virtual memory parameters: page size influences TLB coverage and page-fault frequency; \\nTLB size/associativity affects hit rate and lookup time.\\n7.4 Summary of Section 7'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35'}, page_content='7.4 Summary of Section 7\\n• Each level acts as a cache for the next level down.\\n• Virtual memory extends the hierarchy to secondary storage and isolates processes.\\n• Address translation (MMU and TLB) precedes cache lookup in typical designs.\\n• Good locality improves both cache and TLB hit rates.\\n• The same core ideas recur at all levels: placement (mapping), identification (tags), replace\\xad\\nment, and write/consistency policies.\\nA Reference Tables\\nA.1 MIPS vs x86_64 Addressing Summary'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35'}, page_content='A.1 MIPS vs x86_64 Addressing Summary\\n Concept  MIPS  x86_64 (AT&T)\\n Memory operand form  offset($base)  disp(%base,%index,scale)\\n Example  lw $t0, 8($s1)  movq 8(%rsi), %rax\\n Base + displacement  Yes  Yes\\n Index register  Computed in ALU  Built-in (%index)\\n Scale factor  1 only  1, 2, 4, 8\\n Load/store discipline  Load/store architecture  Memory operands in many ops\\n Typical array access  lw $t0, 4($s1)  movl (%rdi,%rcx,4), %eax\\nA.2 Glossary'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35'}, page_content='A.2 Glossary\\nAMAT Average Memory Access Time; hit time + miss rate × miss penalty.\\nBlock (cache line) Fixed-size unit transferred between cache and memory.\\nFrame Fixed-size physical memory block for a virtual page.\\nLocality Temporal: reuse over time; Spatial: reuse of nearby addresses.\\nMMU Memory Management Unit; performs virtual-to-physical translation.\\nPage Fixed-size unit of virtual memory; size commonly 4 KB.\\nPTE Page Table Entry; maps VPN to PFN and stores permissions and valid/dirty bits.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35'}, page_content='TLB Translation Lookaside Buffer; small associative cache of recent VPN to PFN mappings.\\nVPN/PFN Virtual Page Number / Physical Frame Number.\\n35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 35, 'page_label': '36'}, page_content='Answer key (final numbers only; not examinable)\\nSection 3 (Caches)\\n3.1 Tag/index/offset for 4 KB DM, 16 B blocks, 32-bit: offset 4, index 8, tag 20.\\n3.2 With sequence 0x0000, 0x0800, 0x1000, 0x0000, 0x1000: miss, miss, miss, miss, hit.\\n3.3 2-way, parameters: sets = 4096/(16 · 2) = 128 ⇒ offset 4, index 7, tag 21; trace as written.\\n3.4 1 KiB DM, 16 B blocks, 16-bit: offset 4, index 6, tag 6; 0xABCD ⇒ tag 0x2A, index 0x3C, offset \\n0xD.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 35, 'page_label': '36'}, page_content='0xD.\\nAMAT example: 1 + 0.05 · 50 = 3.5 cycles; improved 1 + 0.02 · 50 = 2.0 cycles.\\nSection 6 (Virtual Memory)\\n6.1 VA 0x15A3 ⇒ PA 0x08A3.\\n6.2 TLB hit for VPN 5 ⇒ PFN 2 ⇒ PA 0x08A3.\\n6.3 EMAT = (0.95)(1 + 100) + (0.05)(1 + 200) = 106 ns.\\n6.4 EMAT formula: EMAT = h · (tTLB + tmem) + (1 − h) · (tTLB + 2tmem).\\nNumbers: h = 0.97, tTLB = 2 ns, tmem = 120 ns ⇒ EMAT = 0.97·122+0.03·242 = 125.6 ns. Speed-\\nup to h = 0.985: new EMAT = 0.985·122+0.015·242 = 123.8 ns; speed-up ≈ (125.6−123.8)/125.6 ≈'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 35, 'page_label': '36'}, page_content='1.4%. Frames for 18-bit PA, 4 KB pages: 218/212 = 26 = 64 frames; capacity = 64 ·4 KB = 256 KB.\\n36')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# final_documents = text_splitter.create_documents(docs)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "final_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb143648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Figure 2: Every pair of levels in the memory hierarchy can be thought of as having an \n",
      "upper and lower level. Within each level, the unit of information that is present or not is called \n",
      "a block or a line. Usually we transfer an entire block when we copy something between levels. (Fig \n",
      "5.2 in [Patterson & Hennessy])\n",
      "stay near the processor, the system will behave as if all memory were fast.\n",
      "1.2 The Principle of Locality' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 3, 'page_label': '4'}\n",
      "page_content='1.2 The Principle of Locality\n",
      "The idea that makes the hierarchy work is the principle of locality. Empirically, programs do not \n",
      "access memory randomly; instead, they tend to use the same data and nearby data for extended \n",
      "periods.\n",
      "4' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'C:\\\\Users\\\\Karan\\\\Desktop\\\\agentic-ai-langchain-langgraph\\\\LangChain\\\\1. LangChain\\\\DataIngestion\\\\computer_systems_lect.pdf', 'total_pages': 36, 'page': 3, 'page_label': '4'}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[15])\n",
    "print(final_documents[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ba98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
